{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETHIC - Primavera 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Selección de caso\n",
    "\n",
    "* 1) Caso Amanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.extract_data import process_data\n",
    "from utils.extract_data import read_data\n",
    "\n",
    "caso = process_data()\n",
    "df1, df2 = read_data(caso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.head()\n",
    "df2 = df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis Gramatical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se debe ejecutar la celda de ejecutar análisis solo si se desea realizar uno nuevo. De lo contrario basta con ejecutar la celda de graficar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dependency_tree import ejecutar_analisis\n",
    "ejecutar_analisis(df1, df2, caso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from utils.dependency_tree import tree_dependency_graphs\n",
    "tree_dependency_graphs(caso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis de Topicos BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento Modelo Bert Topic\n",
    "\n",
    "Solo es necesario ejecutarlo si se desea entrenar nuevamente al modelo con más datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.bertopic_model import *\n",
    "\n",
    "# Crear tokenizer\n",
    "tokenizer = StemmerTokenizer(stem=False, lemmatize=True)\n",
    "\n",
    "# Cargar y preprocesar comentarios\n",
    "comentarios_tokenizados = cargar_y_preprocesar_comentarios(df1, df2, tokenizer)\n",
    "\n",
    "# Entrenar modelo BERTopic\n",
    "entrenar_modelo_bertopic(comentarios_tokenizados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tópicos BERT\n",
    "\n",
    "Obtener los tópicos encontrados por el modelo BERT. Esta celda se debe ejecutar solo si se desea realizar un nuevo análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.bertopic_model import *\n",
    "# Cargar el modelo BERTopic desde el archivo guardado\n",
    "BERT_model = BERTopic.load(\"models/BertTopic_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediccion de topicos (ejecutar si se desea realizar uha nueva)\n",
    "predict_topics(BERT_model, df1, df2, caso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "caso = \"Amanda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando proceso para el diferencial 1...\n",
      "Realizando conteo de tópicos...\n",
      "Generando gráfico de Frencuencia de Tópicos por Etapa...\n",
      "Gráfico generado exitosamente.\n",
      "Generando gráficos de tópicos más y menos comunes...\n",
      "Gráficos generados exitosamente.\n",
      "Iniciando proceso para el diferencial 2...\n",
      "Realizando conteo de tópicos...\n",
      "Generando gráfico de Frencuencia de Tópicos por Etapa...\n",
      "Gráfico generado exitosamente.\n",
      "Generando gráficos de tópicos más y menos comunes...\n",
      "Gráficos generados exitosamente.\n"
     ]
    }
   ],
   "source": [
    "topic_counts_resultados = contar_topicos(caso)\n",
    "graficar_topicos_agrupados(topic_counts_resultados, 1, BERT_model, caso) \n",
    "graficar_topicos_agrupados(topic_counts_resultados, 2, BERT_model, caso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de Tópics Éticos\n",
    "\n",
    "La celda para obtener los tópicos éticos solo debe ejecutarse si se desea realizar una nueva predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ethic_topics_model import predict_ethic_topic\n",
    "predict_ethic_topic(df1, df2, caso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtienen gráficos de los tópicos éticos encontrados, y aquellos que son más y menos comunes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from utils.ethic_topics_model import procesar_y_graficar_topicos\n",
    "procesar_y_graficar_topicos(caso, 1)\n",
    "procesar_y_graficar_topicos(caso, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tópicos distintos entre etapas\n",
    "\n",
    "Análisis de cuantos topicos distintos se encuentran en las respuestas, diferenciando por etapas y por tópicos BERT y ETHIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ethic_words import distinct_topics\n",
    "distinct_topics(caso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nubes de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from utils.bertopic_model import *\n",
    "from utils.wordclouds import *\n",
    "\n",
    "stop_words_custom\n",
    "tokenizer = StemmerTokenizer(stem=True, lemmatize=True)\n",
    "crear_nubes_palabras(caso, df1, df2, tokenizer=tokenizer, stop_words_custom=stop_words_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frecuencia de aparación de palabras éticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.bertopic_model import *\n",
    "from utils.ethic_words import *\n",
    "\n",
    "stop_words_custom\n",
    "tokenizer = StemmerTokenizer(stem=True, lemmatize=True)\n",
    "contar_palabras_etica(df1, df2, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conectores más usados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Justificación ética post conectores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ethic_unit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
