{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicación de análisis del caso\n",
    "\n",
    "Casos disponibles:\n",
    "- Amanda 2024 [Realizado]\n",
    "- Amanda 2023 [Realizado]\n",
    "- Adela 2024 [Realizado]\n",
    "- Adela 2023 [Realizado]\n",
    "- Adela 2022 [Falta comprobar consentimiento]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "caso = \"Adela 2024\"\n",
    "resultados_img = f\"../resultados/{caso}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar datos del caso por sección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caso seleccionado: Adela 2024\n",
      "Archivo: Procesamient_2024_CD1201_11.xlsx, Año: 2024, Sección: 11\n",
      "Archivo: Procesamient_2024_CD1201_14.xlsx, Año: 2024, Sección: 14\n",
      "Archivo: Procesamient_2024_CD1201_15.xlsx, Año: 2024, Sección: 15\n",
      "Archivo: Procesamient_2024_CD1201_16.xlsx, Año: 2024, Sección: 16\n",
      "Archivo: Procesamient_2024_CD1201_17.xlsx, Año: 2024, Sección: 17\n",
      "Archivo: Procesamient_2024_CD1201_18.xlsx, Año: 2024, Sección: 18\n",
      "Archivo: Procesamient_2024_CD1201_19.xlsx, Año: 2024, Sección: 19\n",
      "Archivo: Procesamient_2024_CD1201_3.xlsx, Año: 2024, Sección: 3\n",
      "Archivo: Procesamient_2024_CD1201_4.xlsx, Año: 2024, Sección: 4\n",
      "Archivo: Procesamient_2024_CD1201_6.xlsx, Año: 2024, Sección: 6\n",
      "\n",
      "Carpeta ya existe: processed_data/Adela 2024\n",
      "Datos procesados guardados en: processed_data/Adela 2024/answers_by_secc_Adela 2024.csv\n",
      "Total de filas: 644 y filas del archivo: 644\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import warnings\n",
    "\n",
    "# Ignorar advertencias específicas de openpyxl\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "\n",
    "print(f\"Caso seleccionado: {caso}\")\n",
    "\n",
    "# PATH a las carpetas de datos\n",
    "ANSWERS_DATA_PATH = f'../data/answers/{caso}'\n",
    "FOLDERS_ANSWERS = os.listdir(ANSWERS_DATA_PATH)\n",
    "\n",
    "# Dataframe para almacenar los datos\n",
    "df_answers = pd.DataFrame(columns=['df', 'opt_left', 'Grup', 'Ind1', 'Ind2', 'Magnitud_Ind1_Grup', 'Magnitud_Grup_Ind2', \n",
    "                                'Magnitud_Ind1_Ind2', 'Cambio_postura_Ind1_Grup', 'Cambio_postura_Grup_Ind2', \n",
    "                                'Cambio_postura_Ind1_Ind2', 'Nivel_Ind1_Grup', 'Nivel_Grup_Ind2', 'Nivel_Ind1_Ind2', \n",
    "                                'Direccion_Ind1_Grup', 'Direccion_Grup_Ind2', 'Direccion_Ind1_Ind2', \n",
    "                                'Comentario - Ind1 - Diferencial 1', 'Comentario - Ind1 - Diferencial 2', \n",
    "                                'Comentario - Grup - Diferencial 1', 'Comentario - Grup - Diferencial 2', \n",
    "                                'Comentario - Ind2 - Diferencial 1', 'Comentario - Ind2 - Diferencial 2', 'agno', 'seccion'])\n",
    "\n",
    "# Leer archivos excel del directorio y extraer agno y seccion\n",
    "filas = 0\n",
    "for file in FOLDERS_ANSWERS:\n",
    "    parts = file.split('_')  \n",
    "    if len(parts) >= 3: \n",
    "        agno = parts[1]  \n",
    "        seccion = parts[-1][:-5] \n",
    "        print(f\"Archivo: {file}, Año: {agno}, Sección: {seccion}\")\n",
    "    else:\n",
    "        print(f\"==== Error en extracción de año y sección para el archivo: {file} ====\")\n",
    "        agno = None\n",
    "        seccion = None\n",
    "    \n",
    "    # Extracción de columnas\n",
    "    df = pd.read_excel(f'{ANSWERS_DATA_PATH}/{file}', sheet_name='Datos')\n",
    "    \"\"\"\n",
    "    df = df[['df', 'opt_left', 'Grup', 'Ind1', 'Ind2', 'Magnitud_Ind1_Grup', 'Magnitud_Grup_Ind2', \n",
    "            'Magnitud_Ind1_Ind2', 'Cambio_postura_Ind1_Grup', 'Cambio_postura_Grup_Ind2', \n",
    "            'Cambio_postura_Ind1_Ind2', 'Nivel_Ind1_Grup', 'Nivel_Grup_Ind2', 'Nivel_Ind1_Ind2', \n",
    "            'Direccion_Ind1_Grup', 'Direccion_Grup_Ind2', 'Direccion_Ind1_Ind2', \n",
    "            'Comentario - Ind1 - Diferencial 1', 'Comentario - Ind1 - Diferencial 2', \n",
    "            'Comentario - Grup - Diferencial 1', 'Comentario - Grup - Diferencial 2', \n",
    "            'Comentario - Ind2 - Diferencial 1', 'Comentario - Ind2 - Diferencial 2']]\n",
    "    \"\"\"\n",
    "    df = df[['df', 'opt_left', 'Grup', 'Ind1', 'Ind2',\n",
    "        'Comentario - Ind1 - Diferencial 1', 'Comentario - Ind1 - Diferencial 2', \n",
    "        'Comentario - Grup - Diferencial 1', 'Comentario - Grup - Diferencial 2', \n",
    "        'Comentario - Ind2 - Diferencial 1', 'Comentario - Ind2 - Diferencial 2']]\n",
    "    # Añadir columnas de agno y seccion\n",
    "    df['agno'] = agno\n",
    "    df['seccion'] = seccion\n",
    "    filas += len(df)\n",
    "    df_answers = pd.concat([df_answers, df], ignore_index=True)\n",
    "\n",
    "# Sort, fillna y convertir columnas a string\n",
    "df_answers.sort_values(by=['Grup', 'seccion', 'agno'], inplace=True)\n",
    "df_answers.fillna('', inplace=True)\n",
    "\n",
    "# Convertir las columnas especificadas a tipo string\n",
    "columnas_a_convertir = [\n",
    "    'Comentario - Ind1 - Diferencial 1',\n",
    "    'Comentario - Grup - Diferencial 1',\n",
    "    'Comentario - Ind2 - Diferencial 1',\n",
    "    'Comentario - Ind1 - Diferencial 2',\n",
    "    'Comentario - Grup - Diferencial 2',\n",
    "    'Comentario - Ind2 - Diferencial 2'\n",
    "]\n",
    "\n",
    "# Convertir las columnas a string\n",
    "for columna in columnas_a_convertir:\n",
    "    df_answers[columna] = df_answers[columna].astype(str)\n",
    "\n",
    "# Ruta para almacenar los datos procesados\n",
    "folder_path = f'processed_data/{caso}'\n",
    "\n",
    "# Verificar si la carpeta existe, y si no, crearla\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "    print(f\"\\nCarpeta creada: {folder_path}\")\n",
    "else:\n",
    "    print(f\"\\nCarpeta ya existe: {folder_path}\")\n",
    "\n",
    "# Guardar datos como un nuevo csv en una ruta\n",
    "df_answers.to_csv(f'../processed_data/{caso}/answers_by_secc_{caso}.csv', index=False)\n",
    "print(f\"Datos procesados guardados en: {folder_path}/answers_by_secc_{caso}.csv\")\n",
    "print(f\"Total de filas: {filas} y filas del archivo: {len(df_answers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "folder_path = f'processed_data/{caso}'\n",
    "df_caso = pd.read_csv(f'../{folder_path}/answers_by_secc_{caso}.csv', sep=',')\n",
    "\n",
    "# Reducir a 20 filas\n",
    "#df_caso = df_caso.head(20).sort_values(by=['agno', 'seccion', 'Grup'])\n",
    "\n",
    "df_caso = df_caso.sort_values(by=['Grup', 'seccion', 'agno'])\n",
    "df_caso = df_caso.fillna('')\n",
    "\n",
    "# Columnas con respuestas\n",
    "columnas_a_convertir = [\n",
    "    'Comentario - Ind1 - Diferencial 1',\n",
    "    'Comentario - Grup - Diferencial 1',\n",
    "    'Comentario - Ind2 - Diferencial 1',\n",
    "    'Comentario - Ind1 - Diferencial 2',\n",
    "    'Comentario - Grup - Diferencial 2',\n",
    "    'Comentario - Ind2 - Diferencial 2'\n",
    "]\n",
    "\n",
    "# Convertir las columnas a string\n",
    "for columna in columnas_a_convertir:\n",
    "    df_caso[columna] = df_caso[columna].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distinción por diferenciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar por diferencial 1 y diferencial 2\n",
    "# diferencial 1 conserva las columnas: 'Grup', 'Comentario - Ind1 - Diferencial 1', 'Comentario - Grup - Diferencial 1', 'Comentario - Ind2 - Diferencial 1' para df=1\n",
    "df_df1 = df_caso[df_caso['df'] == 1][['Grup', \n",
    "                                   'Comentario - Ind1 - Diferencial 1', \n",
    "                                   'Comentario - Grup - Diferencial 1', \n",
    "                                   'Comentario - Ind2 - Diferencial 1', 'agno', 'seccion']]\n",
    "\n",
    "# diferencial 2 conserva las columnas: 'Grup', 'Comentario - Ind1 - Diferencial 2', 'Comentario - Grup - Diferencial 2', 'Comentario - Ind2 - Diferencial 2' para df=2\n",
    "df_df2 = df_caso[df_caso['df'] == 2][['Grup', \n",
    "                                   'Comentario - Ind1 - Diferencial 2', \n",
    "                                   'Comentario - Grup - Diferencial 2', \n",
    "                                   'Comentario - Ind2 - Diferencial 2', 'agno', 'seccion']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis Gramatical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Importar las funciones del archivo software.py ubicado en la carpeta software_development\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from model__1 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicar_dependencias_grupal(df, diferencial, col_ind1, col_grup, col_ind2):\n",
    "    df['Ind1_d'] = df[col_ind1].apply(lambda x: analizar_dependencias(str(x)))\n",
    "    df['Ind2_d'] = df[col_ind2].apply(lambda x: analizar_dependencias(str(x)))\n",
    "    \n",
    "    grouped = df[['Grup', 'agno', 'seccion', col_grup]].drop_duplicates(subset=['Grup', 'agno', 'seccion'])\n",
    "    grouped['Grup_d'] = grouped[col_grup].apply(lambda x: analizar_dependencias(str(x)))\n",
    "    df = df.merge(grouped[['Grup', 'agno', 'seccion', 'Grup_d']], \n",
    "                  on=['Grup', 'agno', 'seccion'], \n",
    "                  how='left')\n",
    "    \n",
    "    df.to_csv(f\"../processed_data/{caso}/Tree_dependency_df{diferencial}.csv\", index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicar y guardar análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar la función optimizada a ambos DataFrames\n",
    "Tree_Dependency_d1 = aplicar_dependencias_grupal(df_df1, 1, 'Comentario - Ind1 - Diferencial 1', \n",
    "                                               'Comentario - Grup - Diferencial 1', \n",
    "                                               'Comentario - Ind2 - Diferencial 1')\n",
    "\n",
    "Tree_Dependency_d2 = aplicar_dependencias_grupal(df_df2, 2, 'Comentario - Ind1 - Diferencial 2', \n",
    "                                               'Comentario - Grup - Diferencial 2', \n",
    "                                               'Comentario - Ind2 - Diferencial 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráficos de analisis gramatical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En caso de no ejecutar lo anterior, se pueden cargar los datos desde los csv\n",
    "import pandas as pd\n",
    "Tree_Dependency_d1 = pd.read_csv(f\"../processed_data/{caso}/Tree_dependency_df1.csv\")\n",
    "Tree_Dependency_d2 = pd.read_csv(f\"../processed_data/{caso}/Tree_dependency_df2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Total de alumnos en diferencial 1: 323\n",
      "Total de alumnos en diferencial 2: 321\n",
      "Total de respuestas en diferencial 1: 969\n",
      "Total de respuestas en diferencial 2: 963\n",
      "-------------------------------------------\n",
      "Generando gráficos...\n",
      "Gráfico de bloxplot generado\n",
      "Tabla de conteo generada\n",
      "Gráfico de conteo generado\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================== CALCULOS ============================================== #\n",
    "total_alumnos_df1 = Tree_Dependency_d1.shape[0]\n",
    "total_alumnos_df2 = Tree_Dependency_d2.shape[0]\n",
    "total_respuestas_df1 = total_alumnos_df1 * 3\n",
    "total_respuestas_df2 = total_alumnos_df2 * 3\n",
    "print(\"-------------------------------------------\")\n",
    "print(f\"Total de alumnos en diferencial 1: {total_alumnos_df1}\")\n",
    "print(f\"Total de alumnos en diferencial 2: {total_alumnos_df2}\")\n",
    "print(f\"Total de respuestas en diferencial 1: {total_respuestas_df1}\")\n",
    "print(f\"Total de respuestas en diferencial 2: {total_respuestas_df2}\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "# ============================================== GRAFICOS ============================================== #\n",
    "print(\"Generando gráficos...\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Datos para los boxplots\n",
    "data_d1 = [Tree_Dependency_d1['Ind1_d'], Tree_Dependency_d1['Grup_d'], Tree_Dependency_d1['Ind2_d']]\n",
    "data_d2 = [Tree_Dependency_d2['Ind1_d'], Tree_Dependency_d2['Grup_d'], Tree_Dependency_d2['Ind2_d']]\n",
    "\n",
    "labels = ['Individual 1', 'Grupal', 'Individual 2']\n",
    "positions_d1 = [1, 2, 3]  # Posiciones para diferencial 1\n",
    "positions_d2 = [4, 5, 6]  # Posiciones para diferencial 2\n",
    "\n",
    "# Crear boxplots\n",
    "boxplot_d1 = plt.boxplot(\n",
    "    data_d1,\n",
    "    positions=positions_d1,\n",
    "    widths=0.6,\n",
    "    patch_artist=True,\n",
    "    boxprops=dict(facecolor='lightblue', color='blue'),\n",
    "    whiskerprops=dict(color='blue'),\n",
    "    capprops=dict(color='blue'),\n",
    "    medianprops=dict(color='darkblue'),\n",
    "    flierprops=dict(markeredgecolor='blue')\n",
    ")\n",
    "\n",
    "boxplot_d2 = plt.boxplot(\n",
    "    data_d2,\n",
    "    positions=positions_d2,\n",
    "    widths=0.6,\n",
    "    patch_artist=True,\n",
    "    boxprops=dict(facecolor='lightgreen', color='green'),\n",
    "    whiskerprops=dict(color='green'),\n",
    "    capprops=dict(color='green'),\n",
    "    medianprops=dict(color='darkgreen'),\n",
    "    flierprops=dict(markeredgecolor='green')\n",
    ")\n",
    "\n",
    "# Etiquetas y formato\n",
    "plt.xticks([1, 2, 3, 4, 5, 6], \n",
    "           ['Individual 1', 'Grupal', 'Individual 2', \n",
    "            'Individual 1', 'Grupal', 'Individual 2'])\n",
    "plt.ylabel('Dependencias')\n",
    "plt.title(f'Caso: {caso} - Oraciones subordinadas por etapa y diferencial', fontsize=16, pad=20, fontfamily='serif')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Añadir leyenda\n",
    "plt.legend([boxplot_d1[\"boxes\"][0], boxplot_d2[\"boxes\"][0]], \n",
    "           ['Diferencial 1', 'Diferencial 2'], \n",
    "           loc='upper right', fontsize=12)\n",
    "\n",
    "# Información adicional\n",
    "plt.figtext(\n",
    "    0.5, -0.05, \n",
    "    f'Estudiantes diferencial 1: {total_alumnos_df1}\\nEstudiantes diferencial 2: {total_alumnos_df2}', \n",
    "    ha='center', fontsize=10\n",
    ")\n",
    "\n",
    "# Guardar gráfico\n",
    "plt.savefig(f\"{resultados_img}/Analisis_Gramatical_boxplot_dependencias.png\", bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"Gráfico de bloxplot generado\")\n",
    "# ============================================== GRAFICOS ============================================== #\n",
    "diferenciales = []\n",
    "for i, df in enumerate([Tree_Dependency_d1, Tree_Dependency_d2], start=1):\n",
    "    total_alumnos = total_alumnos_df1 if i == 1 else total_alumnos_df2\n",
    "\n",
    "    ind1_count = (df['Ind1_d'] < df['Ind2_d']).sum()\n",
    "    grup_count = (df['Grup_d'] < df['Ind2_d']).sum()\n",
    "    ind2_count = (df['Ind2_d'] < df['Ind1_d']).sum()\n",
    "    # Porcentajes f\"{num:.1f}\"\n",
    "    ind1_percentage = (ind1_count*100)/total_alumnos\n",
    "    grup_percentage = (grup_count*100)/total_alumnos\n",
    "    ind2_percentage = (ind2_count*100)/total_alumnos\n",
    "    \n",
    "    diferenciales.append([f'{i}', 'Individual 1 < Individual 2', ind1_count, f\"{ind1_percentage:.1f}%\"])\n",
    "    diferenciales.append([f'{i}', 'Grupal < Individual 2', grup_count, f\"{grup_percentage:.1f}%\"])\n",
    "    diferenciales.append([f'{i}', 'Individual 2 < Individual 1', ind2_count, f\"{ind2_percentage:.1f}%\"])\n",
    "\n",
    "df_table = pd.DataFrame(diferenciales, columns=['Diferencial', 'Comparación', 'Conteo', 'Porcentaje'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))  \n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=df_table.values, colLabels=df_table.columns, cellLoc='center', loc='center')\n",
    "table.auto_set_font_size(False)  \n",
    "table.set_fontsize(12)  \n",
    "table.scale(2.0, 2.0)  \n",
    "plt.title(f'Caso: {caso} - Conteo de oraciones subordinadas', fontsize=14, fontfamily='serif')\n",
    "plt.figtext(0.5, -0.05, f'Estudiantes diferencial 1: {total_alumnos_df1}\\nEstudiantes diferencial 2: {total_alumnos_df2}', ha='center', fontsize=10)\n",
    "plt.savefig(f\"{resultados_img}/Analisis_Gramatical_tabla_dependencias.png\", bbox_inches='tight')\n",
    "plt.close() \n",
    "print(\"Tabla de conteo generada\")\n",
    "# ============================================== GRAFICOS ============================================== #\n",
    "diferencial_1_counts = [\n",
    "    (Tree_Dependency_d1['Ind1_d'] < Tree_Dependency_d1['Ind2_d']).sum(),\n",
    "    (Tree_Dependency_d1['Grup_d'] < Tree_Dependency_d1['Ind2_d']).sum(),\n",
    "    (Tree_Dependency_d1['Ind2_d'] < Tree_Dependency_d1['Ind1_d']).sum()\n",
    "]\n",
    "\n",
    "diferencial_2_counts = [\n",
    "    (Tree_Dependency_d2['Ind1_d'] < Tree_Dependency_d2['Ind2_d']).sum(),\n",
    "    (Tree_Dependency_d2['Grup_d'] < Tree_Dependency_d2['Ind2_d']).sum(),\n",
    "    (Tree_Dependency_d2['Ind2_d'] < Tree_Dependency_d2['Ind1_d']).sum()\n",
    "]\n",
    "\n",
    "bar_width = 0.35\n",
    "x = np.arange(len(diferencial_1_counts))  \n",
    "plt.style.use('fivethirtyeight')  \n",
    "plt.figure(figsize=(10, 5))\n",
    "bars1 = plt.bar(x - bar_width/2, diferencial_1_counts, width=bar_width, label='Diferencial 1', color='lightblue', edgecolor='black')\n",
    "bars2 = plt.bar(x + bar_width/2, diferencial_2_counts, width=bar_width, label='Diferencial 2', color='lightgreen', edgecolor='black')\n",
    "plt.ylabel('Conteo', fontsize=12)  \n",
    "plt.title(f'Caso: {caso} - Conteo de oraciones subordinadas', fontsize=14, fontfamily='serif')  \n",
    "plt.xticks(x, ['Individual 1 < Individual 2', 'Grupal < Individual 2', 'Individual 2 < Individual 1'], fontsize=12)  \n",
    "plt.legend(fontsize=12)  \n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7) \n",
    "\n",
    "# Agregar etiquetas con conteo y porcentaje\n",
    "for bar, total in zip(bars1, [total_alumnos_df1] * len(bars1)):\n",
    "    yval = bar.get_height()\n",
    "    percentage = (yval / total) * 100\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, yval, f\"{int(yval)} ({percentage:.1f}%)\", ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "for bar, total in zip(bars2, [total_alumnos_df2] * len(bars2)):\n",
    "    yval = bar.get_height()\n",
    "    percentage = (yval / total) * 100\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, yval, f\"{int(yval)} ({percentage:.1f}%)\", ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "\n",
    "plt.figtext(0.5, -0.05, f'Estudiantes diferencial 1: {total_alumnos_df1}\\nEstudiantes diferencial 2: {total_alumnos_df2}', ha='center', fontsize=10)\n",
    "plt.savefig(f\"{resultados_img}/Analisis_Gramatical_conteo_dependencias.png\", bbox_inches='tight')\n",
    "plt.close() \n",
    "print(\"Gráfico de conteo generado\")\n",
    "print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de topicos BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesar comentarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from nltk.stem import SnowballStemmer\n",
    "from bertopic import BERTopic\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ TOKENIZADOR ============================ #\n",
    "nlp = spacy.load('es_core_news_md')\n",
    "\n",
    "# Función para cargar las stopwords\n",
    "def cargar_stopwords(ruta_archivo):\n",
    "    try:\n",
    "        with open(ruta_archivo, 'r', encoding='utf-8') as f:\n",
    "            return set(f.read().splitlines())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"El archivo {ruta_archivo} no se encontró. Usando un conjunto vacío.\")\n",
    "        return set()\n",
    "\n",
    "global stop_words_custom\n",
    "stop_words_custom = cargar_stopwords('../dictionaries/stopwords_es.txt')\n",
    "\n",
    "# Clase para tokenización y stemming/lemmatización\n",
    "class StemmerTokenizer:\n",
    "    def __init__(self, stem=False, lemmatize=True):\n",
    "        self.stem = stem\n",
    "        self.lemmatize = lemmatize\n",
    "        self.ps = SnowballStemmer('spanish') if stem else None\n",
    "        self.stop_words_custom = cargar_stopwords('../dictionaries/stopwords_es.txt')\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        # Limpiar y procesar el texto\n",
    "        doc = re.sub(r'[^A-Za-záéíóúñÁÉÍÓÚÑ\\s]', '', doc).lower()\n",
    "        spacy_doc = nlp(doc)\n",
    "        tokens = [\n",
    "            self.ps.stem(token.lemma_) if self.stem and self.ps else token.lemma_\n",
    "            for token in spacy_doc \n",
    "            if token.text not in self.stop_words_custom and not token.is_punct\n",
    "        ]\n",
    "        return tokens\n",
    "\n",
    "# Inicializar el tokenizador\n",
    "tokenizer = StemmerTokenizer(stem=False, lemmatize=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================== PROCESAR COMENTARIOS ===================================================== #\n",
    "columnas_comentarios_dif1 = [\n",
    "    'Comentario - Ind1 - Diferencial 1',\n",
    "    'Comentario - Grup - Diferencial 1',\n",
    "    'Comentario - Ind2 - Diferencial 1'\n",
    "]\n",
    "\n",
    "columnas_comentarios_dif2 = [\n",
    "    'Comentario - Ind1 - Diferencial 2',\n",
    "    'Comentario - Grup - Diferencial 2',\n",
    "    'Comentario - Ind2 - Diferencial 2'\n",
    "]\n",
    "\n",
    "# Concatenar los comentarios de las columnas seleccionadas de ambos DataFrames\n",
    "comentarios_dif1 = df_df1[columnas_comentarios_dif1].fillna('').values.flatten()\n",
    "comentarios_dif2 = df_df2[columnas_comentarios_dif2].fillna('').values.flatten()\n",
    "\n",
    "# Unir todos los comentarios en un solo array\n",
    "comentarios = pd.concat([pd.Series(comentarios_dif1), pd.Series(comentarios_dif2)], axis=0).values.flatten()\n",
    "comentarios = [str(c) for c in comentarios]\n",
    "comentarios_tokenizados = [' '.join(tokenizer(c)) for c in comentarios]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 20:28:16,381 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tópicos más importantes:\n",
      "Tópico 0: -1_suplemento_recurso_tradición_importante\n",
      "Tópico 1: 0_vitamina_suplemento_tradición_producir\n",
      "Tópico 2: 1_tradición_identitaria_resguardar_salud\n",
      "Tópico 3: 2_recurso_natural_preservar_comunidad\n",
      "Tópico 4: 3_agua_hídrico_escasez_recurso\n",
      "Tópico 5: 4_niño_anciano_salud_beneficiar\n",
      "Tópico 6: 5_aa_mismiñir_cautela_llego\n",
      "Tópico 7: 6_bacteria_cultivo_ambiente_valer\n",
      "Tópico 8: 7_grupo_votar_cambiar_opinión\n",
      "Tópico 9: 8_cambiar_cambio_opinion_opinión\n",
      "Tópico 10: 9_chile_cultura_diverso_pueblo\n",
      "Tópico 11: 10_cercano_km_superficie_buscar\n",
      "Tópico 12: 11_vida_video_memoria_calidad\n",
      "Tópico 13: 12_acuerdo_aledaña_finalmente_acordar\n",
      "Tópico 14: 13_sequia_facil_prodrir_complicación\n",
      "Tópico 15: 14_conversación_supervivencia_quitar_posible\n",
      "Tópico 16: 15_solución_llenar_generar_raíz\n",
      "Tópico 17: 16_harina_quizas_cantidad_lacteo\n",
      "Tópico 18: 17_conflicto_evitar_experimento_sintonía\n",
      "Tópico 19: 18_partir_mesuradamente_ambos_porcentaje\n",
      "Largo de los tópicos: 20\n"
     ]
    }
   ],
   "source": [
    "# ===================================================== ENTRENAMIENTO MODELO PARA UN CASO ESPECIFICO ===================================================== #\n",
    "model = BERTopic(language=\"spanish\", min_topic_size=2, nr_topics=20, top_n_words=5)\n",
    "topics, probs = model.fit_transform(comentarios_tokenizados)\n",
    "model.save(f\"../saved_models/BertTopic_model_{caso}\")\n",
    "\n",
    "print(\"Tópicos más importantes:\")\n",
    "for i, topic in enumerate(model.get_topic_info().head(20)['Name']):\n",
    "    print(f\"Tópico {i}: {topic}\")\n",
    "\n",
    "print(\"Largo de los tópicos:\", len(model.get_topic_info()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npath = \"../processed_data/Entrenamiento BERT/BERT_training_data.csv\"\\ndf = pd.read_csv(path)\\ncomentarios = df[\\'Respuestas\\'].fillna(\\'\\').values.flatten()\\ncomentarios = [str(c) for c in comentarios]\\ncomentarios_tokenizados = [\\' \\'.join(tokenizer(c)) for c in comentarios]\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===================================================== ENTRENAMIENTO MODELO CON TODOS LOS DATOS ===================================================== #\n",
    "\"\"\"\n",
    "path = \"../processed_data/Entrenamiento BERT/BERT_training_data.csv\"\n",
    "df = pd.read_csv(path)\n",
    "comentarios = df['Respuestas'].fillna('').values.flatten()\n",
    "comentarios = [str(c) for c in comentarios]\n",
    "comentarios_tokenizados = [' '.join(tokenizer(c)) for c in comentarios]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel = BERTopic(language=\"spanish\", min_topic_size=10, nr_topics=20, top_n_words=10)\\ntopics, probs = model.fit_transform(comentarios_tokenizados)\\nmodel.save(f\"../saved_models/BertTopic_model\")\\n\\nprint(\"Tópicos más importantes:\")\\nfor i, topic in enumerate(model.get_topic_info().head(20)[\\'Name\\']):\\n    print(f\"Tópico {i}: {topic}\")\\n\\nprint(\"Largo de los tópicos:\", len(model.get_topic_info()))\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "model = BERTopic(language=\"spanish\", min_topic_size=10, nr_topics=20, top_n_words=10)\n",
    "topics, probs = model.fit_transform(comentarios_tokenizados)\n",
    "model.save(f\"../saved_models/BertTopic_model\")\n",
    "\n",
    "print(\"Tópicos más importantes:\")\n",
    "for i, topic in enumerate(model.get_topic_info().head(20)['Name']):\n",
    "    print(f\"Tópico {i}: {topic}\")\n",
    "\n",
    "print(\"Largo de los tópicos:\", len(model.get_topic_info()))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Datos de los tópicos (extraídos del modelo)\n",
    "num_topicos = len(model.get_topic_info())\n",
    "topics = model.get_topic_info().head(20)['Name']\n",
    "\n",
    "# Crear un DataFrame para estructurar los datos\n",
    "data = {\"Tópico\": [f\"Tópico {i}\" for i in range(num_topicos)], \"Palabras\": topics}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Crear la figura y la tabla con fondo blanco\n",
    "fig, ax = plt.subplots(figsize=(6, 6), facecolor=\"white\")\n",
    "ax.axis(\"off\")  # Ocultar los ejes\n",
    "ax.axis(\"tight\")  # Ajustar el tamaño al contenido\n",
    "table = ax.table(cellText=df.values, colLabels=df.columns, loc=\"center\", cellLoc=\"center\")\n",
    "\n",
    "# Ajustar el estilo de la tabla\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.auto_set_column_width(col=list(range(len(df.columns))))\n",
    "\n",
    "# Ajustar el título con menos separación\n",
    "plt.subplots_adjust(top=1.0)  # Controlar el espacio entre la tabla y el título\n",
    "plt.suptitle(f\"Tópicos BERT más importantes - {caso}\", fontsize=16, fontweight='bold', fontfamily='serif')\n",
    "\n",
    "# Guardar la imagen con fondo blanco\n",
    "plt.savefig(f\"{resultados_img}/BERT_TOPICS_TABLE.png\", dpi=300, bbox_inches=\"tight\", facecolor=fig.get_facecolor())\n",
    "plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================== PREDICCIÓN TÓPICOS ===================================================== #\n",
    "model = BERTopic.load(f\"../saved_models/BertTopic_model_{caso}\")\n",
    "\n",
    "# Extraer tópicos para cada etapa (Ind1, Grup, Ind2)\n",
    "topics_ind1_dif1 = model.transform(df_df1['Comentario - Ind1 - Diferencial 1'].values.flatten())[0]\n",
    "topics_grup_dif1 = model.transform(df_df1['Comentario - Grup - Diferencial 1'].values.flatten())[0]\n",
    "topics_ind2_dif1 = model.transform(df_df1['Comentario - Ind2 - Diferencial 1'].values.flatten())[0]\n",
    "\n",
    "topics_ind1_dif2 = model.transform(df_df2['Comentario - Ind1 - Diferencial 2'].values.flatten())[0]\n",
    "topics_grup_dif2 = model.transform(df_df2['Comentario - Grup - Diferencial 2'].values.flatten())[0]\n",
    "topics_ind2_dif2 = model.transform(df_df2['Comentario - Ind2 - Diferencial 2'].values.flatten())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar los tópicos en los dataframes\n",
    "BERT_df1 = df_df1.copy()\n",
    "BERT_df2 = df_df2.copy()\n",
    "\n",
    "BERT_df1['BERT_topicos_ind1'] = topics_ind1_dif1\n",
    "BERT_df1['BERT_topicos_grup'] = topics_grup_dif1\n",
    "BERT_df1['BERT_topicos_ind2'] = topics_ind2_dif1\n",
    "\n",
    "BERT_df2['BERT_topicos_ind1'] = topics_ind1_dif2\n",
    "BERT_df2['BERT_topicos_grup'] = topics_grup_dif2\n",
    "BERT_df2['BERT_topicos_ind2'] = topics_ind2_dif2\n",
    "\n",
    "# Guardar como csv\n",
    "BERT_df1.to_csv(f\"../processed_data/{caso}/BERT_df1.csv\", index=False)\n",
    "BERT_df2.to_csv(f\"../processed_data/{caso}/BERT_df2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar csv con los tópicos\n",
    "BERT_df1 = pd.read_csv(f\"../processed_data/{caso}/BERT_df1.csv\")\n",
    "BERT_df2 = pd.read_csv(f\"../processed_data/{caso}/BERT_df2.csv\")\n",
    "\n",
    "topics_ind1_dif1 = BERT_df1['BERT_topicos_ind1']\n",
    "topics_grup_dif1 = BERT_df1['BERT_topicos_grup']\n",
    "topics_ind2_dif1 = BERT_df1['BERT_topicos_ind2']\n",
    "\n",
    "topics_ind1_dif2 = BERT_df2['BERT_topicos_ind1']\n",
    "topics_grup_dif2 = BERT_df2['BERT_topicos_grup']\n",
    "topics_ind2_dif2 = BERT_df2['BERT_topicos_ind2']\n",
    "\n",
    "# Cargar modelo\n",
    "model = BERTopic.load(f\"../saved_models/BertTopic_model_{caso}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para contar tópicos en una lista de comentarios\n",
    "def contar_topicos(topics):\n",
    "    topic_counts = Counter(topics)\n",
    "    return topic_counts\n",
    "\n",
    "# Contar la frecuencia de tópicos en cada grupo\n",
    "topic_counts_ind1_dif1 = contar_topicos(topics_ind1_dif1)\n",
    "topic_counts_grup_dif1 = contar_topicos(topics_grup_dif1)\n",
    "topic_counts_ind2_dif1 = contar_topicos(topics_ind2_dif1)\n",
    "\n",
    "topic_counts_ind1_dif2 = contar_topicos(topics_ind1_dif2)\n",
    "topic_counts_grup_dif2 = contar_topicos(topics_grup_dif2)\n",
    "topic_counts_ind2_dif2 = contar_topicos(topics_ind2_dif2)\n",
    "\n",
    "bert_alumnos_d1 = len(BERT_df1)\n",
    "bert_alumnos_d2 = len(BERT_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando gráfico de Frencuencia de Tópicos por Etapa, diferencial 1...\n",
      "Gráfico generado exitosamente.\n",
      "Generando gráficos de tópicos más y menos comunes, diferencial 1...\n",
      "Gráfico generado exitosamente.\n",
      "\n",
      "Generando gráfico de Frencuencia de Tópicos por Etapa, diferencial 2...\n",
      "Gráfico generado exitosamente.\n",
      "Generando gráficos de tópicos más y menos comunes, diferencial 2...\n",
      "Gráfico generado exitosamente.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "def graficar_topicos_agrupados(topic_counts_ind1, topic_counts_grup, topic_counts_ind2, differential):\n",
    "    all_topics = set(topic_counts_ind1.keys()).union(set(topic_counts_grup.keys()), set(topic_counts_ind2.keys()))\n",
    "    \n",
    "    filtered_topics = [\n",
    "        topic for topic in all_topics \n",
    "        if (topic_counts_ind1.get(topic, 0) + topic_counts_grup.get(topic, 0) + topic_counts_ind2.get(topic, 0)) > 10\n",
    "    ]\n",
    "\n",
    "    top_words = []\n",
    "    for topic in filtered_topics:\n",
    "        try:\n",
    "            top_words.append(\", \".join([w[0] for w in model.get_topic(topic)[:5]]))\n",
    "        except:\n",
    "            top_words.append(f\"Tópico {topic} no encontrado\")\n",
    "\n",
    "    freqs_ind1 = [topic_counts_ind1.get(topic, 0) for topic in filtered_topics]\n",
    "    freqs_grup = [topic_counts_grup.get(topic, 0) for topic in filtered_topics]\n",
    "    freqs_ind2 = [topic_counts_ind2.get(topic, 0) for topic in filtered_topics]\n",
    "\n",
    "    x = range(len(filtered_topics))\n",
    "    print(f\"Generando gráfico de Frencuencia de Tópicos por Etapa, diferencial {differential}...\")\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    width = 0.2\n",
    "     # Graficar barras\n",
    "    bars_ind1 = plt.bar([p - width for p in x], freqs_ind1, width=width, label='Individual 1', color=sns.color_palette(\"Blues\")[2])\n",
    "    bars_grup = plt.bar(x, freqs_grup, width=width, label='Grupal', color=sns.color_palette(\"Greens\")[2])\n",
    "    bars_ind2 = plt.bar([p + width for p in x], freqs_ind2, width=width, label='Individual 2', color=sns.color_palette(\"Oranges\")[2])\n",
    "\n",
    "    plt.xlabel('Tópicos (Palabras Clave)', fontsize=14)\n",
    "    plt.ylabel('Frecuencia', fontsize=14)\n",
    "    plt.title(f\"Caso: {caso} - Frecuencia de Tópicos por Etapa, Diferencial {differential}\", fontsize=16, fontfamily='serif', pad=20, fontweight='bold')\n",
    "    plt.xticks(ticks=x, labels=top_words, rotation=90, fontsize=12)\n",
    "    plt.legend(fontsize=12, loc='upper right')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Añadir porcentajes sobre las barras\n",
    "    alumnos_totales = bert_alumnos_d1 if differential == 1 else bert_alumnos_d2\n",
    "    for bar, freq, total_alumnos in zip(bars_ind1, freqs_ind1, [alumnos_totales]*len(filtered_topics)):\n",
    "        porcentaje = freq * 100 / total_alumnos if total_alumnos > 0 else 0\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5, f\"{porcentaje:.1f}%\", \n",
    "                 ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    for bar, freq, total_alumnos in zip(bars_grup, freqs_grup, [alumnos_totales]*len(filtered_topics)):\n",
    "        porcentaje = freq * 100 / total_alumnos if total_alumnos > 0 else 0\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5, f\"{porcentaje:.1f}%\", \n",
    "                 ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    for bar, freq, total_alumnos in zip(bars_ind2, freqs_ind2, [alumnos_totales]*len(filtered_topics)):\n",
    "        porcentaje = freq * 100 / total_alumnos if total_alumnos > 0 else 0\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5, f\"{porcentaje:.1f}%\", \n",
    "                 ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.figtext(0.5, -0.05, f'Estudiantes diferencial 1: {bert_alumnos_d1}\\nEstudiantes diferencial 2: {bert_alumnos_d2}', \n",
    "                ha='center', fontsize=12)\n",
    "    plt.savefig(f\"{resultados_img}/BERT_frec_topicos_d{differential}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(\"Gráfico generado exitosamente.\")\n",
    "    # ==== GRAFICO TOPICOS MAS COMUNES ==== #\n",
    "    # Top 10 topicos mas frecuentes (considerando todas las etapas y distinguiendo por diferencial)\n",
    "    total_counts = {topic: (topic_counts_ind1.get(topic, 0) + \n",
    "                        topic_counts_grup.get(topic, 0) + \n",
    "                        topic_counts_ind2.get(topic, 0)) \n",
    "                for topic in filtered_topics}\n",
    "\n",
    "    top_10_topics = sorted(total_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    top_10_topics, top_10_freqs = zip(*top_10_topics)\n",
    "\n",
    "    top_words = []\n",
    "    for topic in filtered_topics:\n",
    "        try:\n",
    "            top_words.append(\", \".join([w[0] for w in model.get_topic(topic)[:5]]))\n",
    "        except:\n",
    "            top_words.append(f\"Tópico {topic} no encontrado\")\n",
    "    \n",
    "    top_10_words = [\", \".join([w[0] for w in model.get_topic(topic)[0:5]]) for topic in top_10_topics]\n",
    "    print(f\"Generando gráficos de tópicos más y menos comunes, diferencial {differential}...\")\n",
    "\n",
    "    if differential == 1:\n",
    "        # Lightblue palette\n",
    "        palette = ['#ADD8E6', '#B0E0E6', '#87CEEB', '#87CEFA', '#4682B4']\n",
    "    elif differential == 2:\n",
    "        # Lightgreen palette\n",
    "        palette = ['#90EE90', '#98FB98', '#8FBC8F', '#66CDAA', '#20B2AA']\n",
    "\n",
    "\n",
    "    # Graficar los 10 tópicos más comunes\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    ax = sns.barplot(x=list(top_10_freqs), y=list(top_10_words), palette=palette)\n",
    "\n",
    "    # Calcular el total de alumnos según el diferencial\n",
    "    total_alumnos = bert_alumnos_d1 if differential == 1 else bert_alumnos_d2\n",
    "\n",
    "    # Añadir porcentajes sobre las barras\n",
    "    for bar, freq in zip(ax.patches, top_10_freqs):\n",
    "        porcentaje = freq * 100 / (total_alumnos*3) if total_alumnos > 0 else 0\n",
    "        plt.text(\n",
    "            bar.get_width() + 0.5,  # Posición x\n",
    "            bar.get_y() + bar.get_height() / 2,  # Posición y (centrado en la barra)\n",
    "            f\"{porcentaje:.1f}%\",  # Texto\n",
    "            ha='left', va='center', fontsize=10, color='black'\n",
    "        )\n",
    "\n",
    "    plt.ylabel('Tópicos (Palabras Clave)', fontsize=14)\n",
    "    plt.xlabel('Frecuencia', fontsize=14)\n",
    "    plt.title(f\"Caso: {caso} - Top 10 Tópicos Más Comunes, Diferencial {differential}\", \n",
    "            fontsize=16, fontfamily='serif', pad=20, fontweight='bold')\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.figtext(0.5, -0.05, f'Estudiantes diferencial {differential}: {total_alumnos} | Respuestas: {total_alumnos * 3}', \n",
    "                ha='center', fontsize=12)\n",
    "    plt.savefig(f\"{resultados_img}/BERT_Top10_Topicos_Mas_Comunes_d{differential}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Graficar los 10 tópicos menos comunes\n",
    "    bottom_10_topics = sorted(total_counts.items(), key=lambda x: x[1])[:10]\n",
    "    bottom_10_topics, bottom_10_freqs = zip(*bottom_10_topics)\n",
    "    bottom_10_words = [\", \".join([w[0] for w in model.get_topic(topic)[0:5]]) for topic in bottom_10_topics]\n",
    "\n",
    "    # Crear el gráfico\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    ax = sns.barplot(x=list(bottom_10_freqs), y=list(bottom_10_words), palette=palette)\n",
    "\n",
    "    # Calcular el total de alumnos según el diferencial\n",
    "    total_alumnos = bert_alumnos_d1 if differential == 1 else bert_alumnos_d2\n",
    "\n",
    "    # Añadir porcentajes sobre las barras\n",
    "    for bar, freq in zip(ax.patches, bottom_10_freqs):\n",
    "        porcentaje = freq * 100 / (total_alumnos*3) if total_alumnos > 0 else 0\n",
    "        plt.text(\n",
    "            bar.get_width() + 0.5,  # Posición x\n",
    "            bar.get_y() + bar.get_height() / 2,  # Posición y (centrado en la barra)\n",
    "            f\"{porcentaje:.1f}%\",  # Texto\n",
    "            ha='left', va='center', fontsize=10, color='black'\n",
    "        )\n",
    "\n",
    "    # Configuración del gráfico\n",
    "    plt.ylabel('Tópicos (Palabras Clave)', fontsize=14)\n",
    "    plt.xlabel('Frecuencia', fontsize=14)\n",
    "    plt.title(f\"Caso: {caso} - 10 Tópicos Menos Comunes, Diferencial {differential}\", \n",
    "            fontsize=16, fontfamily='serif', pad=20, fontweight='bold')\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Añadir texto con la cantidad de estudiantes\n",
    "    plt.figtext(0.5, -0.05, f'Estudiantes diferencial {differential}: {total_alumnos} | Respuestas: {total_alumnos * 3}', \n",
    "                ha='center', fontsize=12)\n",
    "\n",
    "    # Guardar el gráfico\n",
    "    plt.savefig(f\"{resultados_img}/BERT_Top10_Topicos_Menos_Comunes_d{differential}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(\"Gráfico generado exitosamente.\\n\")\n",
    "\n",
    "# Graficar para Diferencial 1\n",
    "graficar_topicos_agrupados(topic_counts_ind1_dif1, topic_counts_grup_dif1, topic_counts_ind2_dif1, 1)\n",
    "\n",
    "# Graficar para Diferencial 2\n",
    "graficar_topicos_agrupados(topic_counts_ind1_dif2, topic_counts_grup_dif2, topic_counts_ind2_dif2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de Tópicos Éticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diccionario de tópicos éticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_dict = {\n",
    "    \"discriminación\": [\n",
    "        \"exclusión\",\n",
    "        \"inequidad\",\n",
    "        \"sesgo\",\n",
    "        \"injusticia\",\n",
    "        \"prejuicio\",\n",
    "        \"marginación\",\n",
    "        \"desigualdad\",\n",
    "        \"diversidad\",\n",
    "        \"equidad\",\n",
    "        \"estigmatización\"\n",
    "    ],\n",
    "    \"justicia\": [\n",
    "        \"equidad\",\n",
    "        \"imparcialidad\",\n",
    "        \"derechos\",\n",
    "        \"legalidad\",\n",
    "        \"reparación\",\n",
    "        \"necesidad\",\n",
    "        \"deber\",\n",
    "        \"responsabilidad\",\n",
    "        \"honor\",\n",
    "        \"consenso\"\n",
    "    ],\n",
    "    \"igualdad\": [\n",
    "        \"equidad\",\n",
    "        \"justicia\",\n",
    "        \"inclusión\",\n",
    "        \"derechos\",\n",
    "        \"equilibrio\",\n",
    "        \"paridad\",\n",
    "        \"simetría\",\n",
    "        \"respeto\",\n",
    "        \"diversidad\",\n",
    "        \"acceso\"\n",
    "    ],\n",
    "    \"salud\": [\n",
    "        \"bienestar\",\n",
    "        \"prevención\",\n",
    "        \"cuidado\",\n",
    "        \"atención\",\n",
    "        \"sanidad\",\n",
    "        \"nutrición\",\n",
    "        \"ejercicio\",\n",
    "        \"enfermedad\",\n",
    "        \"rehabilitación\",\n",
    "        \"calidad\"\n",
    "    ],\n",
    "    \"transparencia\": [\n",
    "        \"claridad\",\n",
    "        \"honestidad\",\n",
    "        \"acceso\",\n",
    "        \"información\",\n",
    "        \"integridad\",\n",
    "        \"responsabilidad\",\n",
    "        \"confianza\",\n",
    "        \"divulgación\",\n",
    "        \"visibilidad\",\n",
    "        \"rendición\"\n",
    "    ],\n",
    "    \"responsabilidad\": [\n",
    "        \"deber\",\n",
    "        \"obligación\",\n",
    "        \"compromiso\",\n",
    "        \"rendición\",\n",
    "        \"ética\",\n",
    "        \"lealtad\",\n",
    "        \"confianza\",\n",
    "        \"honestidad\",\n",
    "        \"seriedad\",\n",
    "        \"justicia\"\n",
    "    ],\n",
    "    \"derecho\": [\n",
    "        \"ley\",\n",
    "        \"justicia\",\n",
    "        \"libertad\",\n",
    "        \"igualdad\",\n",
    "        \"protección\",\n",
    "        \"responsabilidad\",\n",
    "        \"derechos humanos\",\n",
    "        \"equidad\",\n",
    "        \"defensa\",\n",
    "        \"legislación\"\n",
    "    ],\n",
    "    \"compromiso\": [\n",
    "        \"dedicación\",\n",
    "        \"lealtad\",\n",
    "        \"responsabilidad\",\n",
    "        \"promesa\",\n",
    "        \"seriedad\",\n",
    "        \"altruismo\",\n",
    "        \"confianza\",\n",
    "        \"honor\",\n",
    "        \"solidaridad\",\n",
    "        \"esfuerzo\"\n",
    "    ],\n",
    "    \"privacidad\": [\n",
    "        \"confidencialidad\",\n",
    "        \"protección\",\n",
    "        \"seguridad\",\n",
    "        \"datos\",\n",
    "        \"control\",\n",
    "        \"información\",\n",
    "        \"anonymidad\",\n",
    "        \"secreto\",\n",
    "        \"derechos\",\n",
    "        \"libertad\"\n",
    "    ],\n",
    "    \"seguridad\": [\n",
    "        \"protección\",\n",
    "        \"prevención\",\n",
    "        \"confianza\",\n",
    "        \"integridad\",\n",
    "        \"estabilidad\",\n",
    "        \"defensa\",\n",
    "        \"resiliencia\",\n",
    "        \"alerta\",\n",
    "        \"evaluación\",\n",
    "        \"cuidado\"\n",
    "    ],\n",
    "    \"migracion\": [\n",
    "        \"movimiento\",\n",
    "        \"asilo\",\n",
    "        \"refugiado\",\n",
    "        \"transición\",\n",
    "        \"integración\",\n",
    "        \"diversidad\",\n",
    "        \"cultura\",\n",
    "        \"identidad\",\n",
    "        \"derechos\",\n",
    "        \"nueva vida\"\n",
    "    ],\n",
    "    \"consideracion\": [\n",
    "        \"respeto\",\n",
    "        \"atención\",\n",
    "        \"cuidado\",\n",
    "        \"evaluación\",\n",
    "        \"empatía\",\n",
    "        \"comprensión\",\n",
    "        \"pensamiento\",\n",
    "        \"reflexión\",\n",
    "        \"sopesar\",\n",
    "        \"valoración\"\n",
    "    ],\n",
    "    \"accesibilidad\": [\n",
    "        \"facilidad\",\n",
    "        \"igualdad\",\n",
    "        \"oportunidad\",\n",
    "        \"inclusión\",\n",
    "        \"derechos\",\n",
    "        \"adaptación\",\n",
    "        \"navegabilidad\",\n",
    "        \"atención\",\n",
    "        \"recursos\",\n",
    "        \"participación\"\n",
    "    ],\n",
    "    \"sesgo\": [\n",
    "        \"prejuicio\",\n",
    "        \"tendencia\",\n",
    "        \"influencia\",\n",
    "        \"discriminación\",\n",
    "        \"desviación\",\n",
    "        \"subjetividad\",\n",
    "        \"preferencia\",\n",
    "        \"injusticia\",\n",
    "        \"parcialidad\",\n",
    "        \"opinión\"\n",
    "    ],\n",
    "    \"credibilidad\": [\n",
    "        \"confianza\",\n",
    "        \"validez\",\n",
    "        \"fiabilidad\",\n",
    "        \"autenticidad\",\n",
    "        \"integridad\",\n",
    "        \"reputación\",\n",
    "        \"veracidad\",\n",
    "        \"seriedad\",\n",
    "        \"prestigio\",\n",
    "        \"honor\"\n",
    "    ],\n",
    "    \"solidaridad\": [\n",
    "        \"apoyo\",\n",
    "        \"unión\",\n",
    "        \"cooperación\",\n",
    "        \"compasión\",\n",
    "        \"empatía\",\n",
    "        \"compromiso\",\n",
    "        \"ayuda\",\n",
    "        \"hermandad\",\n",
    "        \"asistencia\",\n",
    "        \"defensa\"\n",
    "    ],\n",
    "    \"social\": [\n",
    "        \"comunidad\",\n",
    "        \"interacción\",\n",
    "        \"relaciones\",\n",
    "        \"cultura\",\n",
    "        \"participación\",\n",
    "        \"organización\",\n",
    "        \"cohesión\",\n",
    "        \"solidaridad\",\n",
    "        \"justicia\",\n",
    "        \"equidad\"\n",
    "    ],\n",
    "    \"etica\": [\n",
    "        \"moral\",\n",
    "        \"valores\",\n",
    "        \"responsabilidad\",\n",
    "        \"integridad\",\n",
    "        \"justicia\",\n",
    "        \"principios\",\n",
    "        \"honestidad\",\n",
    "        \"conducta\",\n",
    "        \"deber\",\n",
    "        \"normas\"\n",
    "    ],\n",
    "    \"consentimiento\": [\n",
    "        \"acuerdo\",\n",
    "        \"permiso\",\n",
    "        \"autonomía\",\n",
    "        \"voluntad\",\n",
    "        \"información\",\n",
    "        \"libertad\",\n",
    "        \"elección\",\n",
    "        \"participación\",\n",
    "        \"responsabilidad\",\n",
    "        \"aceptación\"\n",
    "    ],\n",
    "    \"vulnerabilidad\": [\n",
    "        \"fragilidad\",\n",
    "        \"riesgo\",\n",
    "        \"exposición\",\n",
    "        \"desprotección\",\n",
    "        \"necesidad\",\n",
    "        \"apoyo\",\n",
    "        \"sensibilidad\",\n",
    "        \"desigualdad\",\n",
    "        \"discapacidad\",\n",
    "        \"marginación\"\n",
    "    ],\n",
    "    \"integridad\": [\n",
    "        \"honestidad\",\n",
    "        \"coherencia\",\n",
    "        \"ética\",\n",
    "        \"valores\",\n",
    "        \"responsabilidad\",\n",
    "        \"rectitud\",\n",
    "        \"transparencia\",\n",
    "        \"fiabilidad\",\n",
    "        \"moralidad\",\n",
    "        \"solidaridad\"\n",
    "    ],\n",
    "    \"inclusion\": [\n",
    "        \"diversidad\",\n",
    "        \"acceso\",\n",
    "        \"participación\",\n",
    "        \"igualdad\",\n",
    "        \"respeto\",\n",
    "        \"aceptación\",\n",
    "        \"justicia\",\n",
    "        \"equidad\",\n",
    "        \"cohesión\",\n",
    "        \"solidaridad\"\n",
    "    ],\n",
    "    \"respeto\": [\n",
    "        \"consideración\",\n",
    "        \"dignidad\",\n",
    "        \"valorización\",\n",
    "        \"honor\",\n",
    "        \"cuidado\",\n",
    "        \"aprecio\",\n",
    "        \"tolerancia\",\n",
    "        \"aceptación\",\n",
    "        \"solidaridad\",\n",
    "        \"responsabilidad\"\n",
    "    ],\n",
    "    \"precaucion\": [\n",
    "        \"cuidado\",\n",
    "        \"prevención\",\n",
    "        \"evaluación\",\n",
    "        \"consideración\",\n",
    "        \"riesgo\",\n",
    "        \"seguridad\",\n",
    "        \"atención\",\n",
    "        \"planificación\",\n",
    "        \"responsabilidad\",\n",
    "        \"conservación\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lematizar el diccionario de palabras clave\n",
    "nlp = spacy.load(\"es_core_news_md\") \n",
    "\n",
    "ethic_topics_keywords = {}\n",
    "\n",
    "tokenizer = StemmerTokenizer(stem=True) \n",
    "for key, values in keywords_dict.items():\n",
    "    lematizado_key = key  \n",
    "    lematizado_values = [tokenizer(value)[0] for value in values]  \n",
    "    ethic_topics_keywords[lematizado_key] = lematizado_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predecir tópicos éticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ethic_palabras_clave_en_comentario(comentario):\n",
    "    ponderaciones = {}\n",
    "    doc = nlp(comentario.lower())\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.is_alpha and token.text not in stop_words_custom:\n",
    "            for key, values in keywords_dict.items():\n",
    "                # Verifica coincidencia directa de la lematización\n",
    "                if token.lemma_ in values:\n",
    "                    ponderaciones[key] = ponderaciones.get(key, 0) + 1\n",
    "\n",
    "    # Si no hay coincidencias, agrega \"Sin tópico\"\n",
    "    if not ponderaciones:\n",
    "        ponderaciones[\"Sin tópico\"] = 1\n",
    "\n",
    "    return ponderaciones\n",
    "\n",
    "def predecir_topicos_eticos(comentario):\n",
    "    ponderaciones = ethic_palabras_clave_en_comentario(comentario)\n",
    "    return [topic for topic, peso in ponderaciones.items() if peso > 0]  \n",
    "\n",
    "def predict_ethic_topic(df1, df2, caso):\n",
    "    df1['ETHIC_topicos_ind1'] = \"\"\n",
    "    df1['ETHIC_topicos_ind2'] = \"\"\n",
    "\n",
    "    df2['ETHIC_topicos_ind1'] = \"\"\n",
    "    df2['ETHIC_topicos_ind2'] = \"\"\n",
    "\n",
    "    if 'ETHIC_topicos_grup' in df1.columns:\n",
    "        df1 = df1.drop(columns=['ETHIC_topicos_grup'])\n",
    "    if 'ETHIC_topicos_grup' in df2.columns:\n",
    "        df2 = df2.drop(columns=['ETHIC_topicos_grup'])\n",
    "\n",
    "    # Definir una función para predecir tópicos de grupos\n",
    "    def predecir_topicos_grup(df, group_cols, comment_col):\n",
    "        grouped = df[group_cols + [comment_col]].drop_duplicates(subset=group_cols)\n",
    "        grouped['ETHIC_topicos_grup'] = grouped[comment_col].apply(lambda x: predecir_topicos_eticos(str(x)))\n",
    "        df = df.merge(grouped[group_cols + ['ETHIC_topicos_grup']], on=group_cols, how='left')\n",
    "        return df\n",
    "\n",
    "    # Topicos grupales\n",
    "    df1 = predecir_topicos_grup(df1, ['Grup', 'seccion', 'agno'], 'Comentario - Grup - Diferencial 1')\n",
    "    df2 = predecir_topicos_grup(df2, ['Grup', 'seccion', 'agno'], 'Comentario - Grup - Diferencial 2')\n",
    "\n",
    "    # Topicos para Ind1 e Ind2\n",
    "    for i, comentario in df1['Comentario - Ind1 - Diferencial 1'].items():\n",
    "        topicos = predecir_topicos_eticos(comentario)\n",
    "        df1.at[i, 'ETHIC_topicos_ind1'] = topicos\n",
    "\n",
    "    for i, comentario in df1['Comentario - Ind2 - Diferencial 1'].items():\n",
    "        topicos = predecir_topicos_eticos(comentario)\n",
    "        df1.at[i, 'ETHIC_topicos_ind2'] = topicos\n",
    "\n",
    "    for i, comentario in df2['Comentario - Ind1 - Diferencial 2'].items():\n",
    "        topicos = predecir_topicos_eticos(comentario)\n",
    "        df2.at[i, 'ETHIC_topicos_ind1'] = topicos\n",
    "\n",
    "    for i, comentario in df2['Comentario - Ind2 - Diferencial 2'].items():\n",
    "        topicos = predecir_topicos_eticos(comentario)\n",
    "        df2.at[i, 'ETHIC_topicos_ind2'] = topicos\n",
    "\n",
    "    # Guardar topicos predichos\n",
    "    df1.to_csv(f\"../processed_data/{caso}/ETHIC_Topics_df1.csv\", index=False)\n",
    "    df2.to_csv(f\"../processed_data/{caso}/ETHIC_Topics_df2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ethic_topic(df_df1, df_df2, caso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Graficar === #\n",
    "def procesar_y_graficar_topicos(caso, differential):\n",
    "    # Leer dataframes\n",
    "    df = pd.read_csv(f\"../processed_data/{caso}/ETHIC_Topics_df{differential}.csv\")\n",
    "    total_alumnos = len(df)\n",
    "\n",
    "    # Aplicar estilo de Seaborn\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    \n",
    "    # Extraer y contar tópicos para cada etapa y diferencial\n",
    "    # Expandir las listas en las columnas y convertir a minúsculas\n",
    "    expanded_topics_ind1 = df['ETHIC_topicos_ind1'].apply(lambda x: eval(x) if isinstance(x, str) else x).explode().str.lower()\n",
    "    expanded_topics_grup = df['ETHIC_topicos_grup'].apply(lambda x: eval(x) if isinstance(x, str) else x).explode().str.lower()\n",
    "    expanded_topics_ind2 = df['ETHIC_topicos_ind2'].apply(lambda x: eval(x) if isinstance(x, str) else x).explode().str.lower()\n",
    "\n",
    "    # Contar las ocurrencias de cada tópico\n",
    "    topic_counts_ind1 = expanded_topics_ind1.value_counts().reset_index()\n",
    "    topic_counts_grup = expanded_topics_grup.value_counts().reset_index()\n",
    "    topic_counts_ind2 = expanded_topics_ind2.value_counts().reset_index()\n",
    "    \n",
    "    # Renombrar las columnas\n",
    "    topic_counts_ind1.columns = ['Tópico', 'Frecuencia']\n",
    "    topic_counts_grup.columns = ['Tópico', 'Frecuencia']\n",
    "    topic_counts_ind2.columns = ['Tópico', 'Frecuencia']\n",
    "    \n",
    "    # Unir todos los conteos en un solo DataFrame\n",
    "    all_counts = pd.merge(topic_counts_ind1, topic_counts_grup, on='Tópico', how='outer', suffixes=('_ind1', '_grup'))\n",
    "    all_counts = pd.merge(all_counts, topic_counts_ind2, on='Tópico', how='outer')\n",
    "    all_counts.columns = ['Tópico', 'Frecuencia_Ind1', 'Frecuencia_Grup', 'Frecuencia_Ind2']\n",
    "    \n",
    "    # Reemplazar NaN por 0\n",
    "    all_counts.fillna(0, inplace=True)\n",
    "\n",
    "    # Preparar las frecuencias totales\n",
    "    all_counts['Frecuencia_Total'] = all_counts['Frecuencia_Ind1'] + all_counts['Frecuencia_Grup'] + all_counts['Frecuencia_Ind2']\n",
    "    \n",
    "    # Obtener los tópicos y frecuencias\n",
    "    top_words = all_counts['Tópico']\n",
    "    freqs_ind1 = all_counts['Frecuencia_Ind1']\n",
    "    freqs_grup = all_counts['Frecuencia_Grup']\n",
    "    freqs_ind2 = all_counts['Frecuencia_Ind2']\n",
    "\n",
    "    porcentajes_ind1 = (freqs_ind1 / total_alumnos) * 100\n",
    "    porcentajes_grup = (freqs_grup / total_alumnos) * 100\n",
    "    porcentajes_ind2 = (freqs_ind2 / total_alumnos) * 100\n",
    "\n",
    "    # Posiciones en el eje X\n",
    "    x = range(len(top_words))\n",
    "    plt.figure(figsize=(18, 8))  \n",
    "\n",
    "    # Ajuste de la posición de las barras\n",
    "    width = 0.3\n",
    "    bar_ind1 = plt.bar([p - width for p in x], freqs_ind1, width=width, label='Individual 1', color=sns.color_palette(\"Blues\")[2])\n",
    "    bar_grup = plt.bar(x, freqs_grup, width=width, label='Grupal', color=sns.color_palette(\"Greens\")[2])\n",
    "    bar_ind2 = plt.bar([p + width for p in x], freqs_ind2, width=width, label='Individual 2', color=sns.color_palette(\"Oranges\")[2])\n",
    "\n",
    "    # Añadir etiquetas y leyenda\n",
    "    plt.xlabel('Tópicos (Palabras Clave)', fontsize=14)\n",
    "    plt.ylabel('Frecuencia', fontsize=14, ha='center')\n",
    "    plt.title(f\"Caso: {caso} - Frecuencia de tópicos por etapa, Diferencial {differential}\", fontsize=16, fontfamily='serif')\n",
    "    plt.xticks(ticks=x, labels=top_words, rotation=90, fontsize=12)\n",
    "    plt.legend(fontsize=12)\n",
    "\n",
    "    # Añadir líneas de cuadrícula\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Añadir los textos sobre las barras (frecuencia y porcentaje), con un pequeño ajuste en la posición vertical\n",
    "    for i, bar in enumerate(bar_ind1):\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2,  # Posición X (centro de la barra)\n",
    "            bar.get_height() + 5,  # Desplazamos los textos hacia arriba\n",
    "            f\"{porcentajes_ind1[i]:.1f}%\",  # Frecuencia y porcentaje\n",
    "            ha='center', va='bottom', fontsize=10, color='black', rotation=90\n",
    "        )\n",
    "    \n",
    "    for i, bar in enumerate(bar_grup):\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2, \n",
    "            bar.get_height() + 5,  # Desplazamos los textos hacia arriba\n",
    "            f\"{porcentajes_grup[i]:.1f}%\",  \n",
    "            ha='center', va='bottom', fontsize=10, color='black', rotation=90\n",
    "        )\n",
    "    \n",
    "    for i, bar in enumerate(bar_ind2):\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2, \n",
    "            bar.get_height() + 5,  # Desplazamos los textos hacia arriba\n",
    "            f\"{porcentajes_ind2[i]:.1f}%\",  \n",
    "            ha='center', va='bottom', fontsize=10, color='black', rotation=90\n",
    "        )\n",
    "\n",
    "    # Guardar el gráfico\n",
    "    plt.figtext(0.75, 0.01, f\"Total de estudiantes: {total_alumnos}\", ha=\"center\", fontsize=12)\n",
    "    plt.savefig(f\"../resultados/{caso}/ETHIC_Topics_Dif{differential}.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Graficar los 10 tópicos más comunes\n",
    "    topico_frecuencia = all_counts[['Tópico', 'Frecuencia_Total']].sort_values(by='Frecuencia_Total', ascending=False).head(10)\n",
    "\n",
    "    # Calcular porcentajes\n",
    "    topico_frecuencia['Porcentaje'] = (topico_frecuencia['Frecuencia_Total'] * 100) / (total_alumnos * 3)\n",
    "\n",
    "    # si el diferencial es 1, color es azul, si es 2, color es verde\n",
    "    color_graph = 'lightblue' if differential == 1 else 'lightgreen'\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    bars = plt.barh(topico_frecuencia['Tópico'], topico_frecuencia['Frecuencia_Total'], color=color_graph)\n",
    "\n",
    "    # Añadir etiquetas con los porcentajes al final de las barras\n",
    "    for i, bar in enumerate(bars):\n",
    "        plt.text(\n",
    "            bar.get_width() + 1,  \n",
    "            bar.get_y() + bar.get_height() / 2, \n",
    "            f\"{topico_frecuencia['Porcentaje'].iloc[i]:.1f}%\", \n",
    "            ha='left', va='center', fontsize=10\n",
    "        )\n",
    "\n",
    "    plt.barh(topico_frecuencia['Tópico'], topico_frecuencia['Frecuencia_Total'], color=color_graph)\n",
    "    plt.xlabel('Ocurrencias en comentarios', fontsize=14)\n",
    "    plt.ylabel('Tópico', fontsize=14)\n",
    "    plt.title(f'Caso: {caso} - Tópicos relevantes más comunes, Diferencial {differential}', fontfamily='serif', fontsize=16)\n",
    "    plt.figtext(0.5, 0.01, f'Total de respuestas: {total_alumnos*3}', ha='center', fontsize=10)\n",
    "    plt.gca().invert_yaxis()  \n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    plt.savefig(f\"../resultados/{caso}/ETHIC_Top10_Topicos_Dif{differential}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Graficar los 10 tópicos menos comunes\n",
    "    topico_frecuencia_menor = all_counts[['Tópico', 'Frecuencia_Total']].sort_values(by='Frecuencia_Total').head(10)\n",
    "\n",
    "    # Calcular porcentajes\n",
    "    topico_frecuencia_menor['Porcentaje'] = (topico_frecuencia_menor['Frecuencia_Total'] * 100) / (total_alumnos * 3)\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    bars = plt.barh(topico_frecuencia_menor['Tópico'], topico_frecuencia_menor['Frecuencia_Total'], color=color_graph)\n",
    "\n",
    "    # Añadir etiquetas con los porcentajes al final de las barras\n",
    "    for i, bar in enumerate(bars):\n",
    "        plt.text(\n",
    "            bar.get_width() + 1,  # Posición X (final de la barra)\n",
    "            bar.get_y() + bar.get_height() / 2,  # Posición Y (centro de la barra)\n",
    "            f\"{topico_frecuencia_menor['Porcentaje'].iloc[i]:.1f}%\",  # Mostrar el porcentaje\n",
    "            ha='left', va='center', fontsize=10\n",
    "        )\n",
    "\n",
    "    plt.barh(topico_frecuencia_menor['Tópico'], topico_frecuencia_menor['Frecuencia_Total'], color=color_graph)\n",
    "    plt.xlabel('Ocurrencias en comentarios', fontsize=14)\n",
    "    plt.ylabel('Tópico', fontsize=14)\n",
    "    plt.title(f'Caso: {caso} - Tópicos relevantes menos comunes, Diferencial {differential}', fontfamily='serif', fontsize=16)\n",
    "    plt.gca().invert_yaxis()  # Invertir el eje Y para mostrar el menos común arriba\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    plt.figtext(0.5, 0.01, f'Total de respuestas: {total_alumnos*3}', ha='center', fontsize=10)\n",
    "    plt.savefig(f\"../resultados/{caso}/ETHIC_Top10_Topicos_Menos_Comunes_Dif{differential}.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'caso' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m procesar_y_graficar_topicos(\u001b[43mcaso\u001b[49m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      2\u001b[0m procesar_y_graficar_topicos(caso, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'caso' is not defined"
     ]
    }
   ],
   "source": [
    "procesar_y_graficar_topicos(caso, 1)\n",
    "procesar_y_graficar_topicos(caso, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tópicos distintos entre etapas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def BERT_contar_topicos_distintos(df, columna):\n",
    "    # Crear un diccionario para almacenar los resultados\n",
    "    topicos_distintos = 0\n",
    "    topicos_distintos += df[columna].nunique()\n",
    "    \n",
    "    return topicos_distintos\n",
    "\n",
    "def ETHIC_contar_topicos_unicos(df, columna):\n",
    "    columna = df[columna].apply(lambda x: ast.literal_eval(x))\n",
    "    # Filtrar los tópicos únicos que no sean \"Sin tópico\"\n",
    "    all_topicos = [topic for topic_list in columna for topic in topic_list if topic != 'Sin tópico']\n",
    "    return len(set(all_topicos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Topicos distintos entre etapas === #\n",
    "def distinct_topics(caso):\n",
    "    # BERT\n",
    "    df1_BERT = pd.read_csv(f'../processed_data/{caso}/BERT_df1.csv')\n",
    "    df2_BERT = pd.read_csv(f'../processed_data/{caso}/BERT_df2.csv')\n",
    "    # ETHICS\n",
    "    df1_ETHICS = pd.read_csv(f'../processed_data/{caso}/ETHIC_Topics_df1.csv')\n",
    "    df2_ETHICS = pd.read_csv(f'../processed_data/{caso}/ETHIC_Topics_df2.csv')\n",
    "\n",
    "    # Contar tópicos únicos en cada columna: ETHIC_topicos_ind1\tETHIC_topicos_ind2\tETHIC_topicos_grup\n",
    "    print('Realizando conteo de tópicos BERT y ETHIC distintos en cada etapa...')\n",
    "    # Diferencial 1\n",
    "    BERT_ind1_df1 = BERT_contar_topicos_distintos(df1_BERT, 'BERT_topicos_ind1')\n",
    "    BERT_grup_df1 = BERT_contar_topicos_distintos(df1_BERT, 'BERT_topicos_grup')\n",
    "    BERT_ind2_df1 = BERT_contar_topicos_distintos(df1_BERT, 'BERT_topicos_ind2')\n",
    "    # Diferencial 2\n",
    "    BERT_ind1_df2 = BERT_contar_topicos_distintos(df2_BERT, 'BERT_topicos_ind1')\n",
    "    BERT_grup_df2 = BERT_contar_topicos_distintos(df2_BERT, 'BERT_topicos_grup')\n",
    "    BERT_ind2_df2 = BERT_contar_topicos_distintos(df2_BERT, 'BERT_topicos_ind2')\n",
    "\n",
    "    # Diferencial 1\n",
    "    ETHIC_ind1_df1 = ETHIC_contar_topicos_unicos(df1_ETHICS, 'ETHIC_topicos_ind1')\n",
    "    ETHIC_grup_df1 = ETHIC_contar_topicos_unicos(df1_ETHICS, 'ETHIC_topicos_grup')\n",
    "    ETHIC_ind2_df1 = ETHIC_contar_topicos_unicos(df1_ETHICS, 'ETHIC_topicos_ind2')\n",
    "    # Diferencial 2\n",
    "    ETHIC_ind1_df2 = ETHIC_contar_topicos_unicos(df2_ETHICS, 'ETHIC_topicos_ind1')\n",
    "    ETHIC_grup_df2 = ETHIC_contar_topicos_unicos(df2_ETHICS, 'ETHIC_topicos_grup')\n",
    "    ETHIC_ind2_df2 = ETHIC_contar_topicos_unicos(df2_ETHICS, 'ETHIC_topicos_ind2')\n",
    "\n",
    "    print(\"Conteo finalizado\")\n",
    "    # Total de topicos distintos\n",
    "    # BERT\n",
    "    total_topicos_BERT = len(set(\n",
    "        df1_BERT['BERT_topicos_ind1'].tolist() +\n",
    "        df1_BERT['BERT_topicos_grup'].tolist() +\n",
    "        df1_BERT['BERT_topicos_ind2'].tolist() +\n",
    "        df2_BERT['BERT_topicos_ind1'].tolist() +\n",
    "        df2_BERT['BERT_topicos_grup'].tolist() +\n",
    "        df2_BERT['BERT_topicos_ind2'].tolist()\n",
    "    ))\n",
    "\n",
    "    # ETHIC\n",
    "    total_topicos_ETHIC = len(set(\n",
    "        [topic for topic_list in df1_ETHICS['ETHIC_topicos_ind1'].apply(ast.literal_eval) for topic in topic_list if topic != 'Sin tópico'] +\n",
    "        [topic for topic_list in df1_ETHICS['ETHIC_topicos_grup'].apply(ast.literal_eval) for topic in topic_list if topic != 'Sin tópico'] +\n",
    "        [topic for topic_list in df1_ETHICS['ETHIC_topicos_ind2'].apply(ast.literal_eval) for topic in topic_list if topic != 'Sin tópico'] +\n",
    "        [topic for topic_list in df2_ETHICS['ETHIC_topicos_ind1'].apply(ast.literal_eval) for topic in topic_list if topic != 'Sin tópico'] +\n",
    "        [topic for topic_list in df2_ETHICS['ETHIC_topicos_grup'].apply(ast.literal_eval) for topic in topic_list if topic != 'Sin tópico'] +\n",
    "        [topic for topic_list in df2_ETHICS['ETHIC_topicos_ind2'].apply(ast.literal_eval) for topic in topic_list if topic != 'Sin tópico']\n",
    "    ))\n",
    "\n",
    "    # Porcentajes\n",
    "    # Calcular porcentajes para BERT\n",
    "    BERT_percentages_df1 = [\n",
    "        (BERT_ind1_df1 * 100 / total_topicos_BERT),\n",
    "        (BERT_grup_df1 * 100 / total_topicos_BERT),\n",
    "        (BERT_ind2_df1 * 100 / total_topicos_BERT),\n",
    "    ]\n",
    "    BERT_percentages_df2 = [\n",
    "        (BERT_ind1_df2 * 100 / total_topicos_BERT),\n",
    "        (BERT_grup_df2 * 100 / total_topicos_BERT),\n",
    "        (BERT_ind2_df2 * 100 / total_topicos_BERT),\n",
    "    ]\n",
    "\n",
    "    # Calcular porcentajes para ETHIC\n",
    "    ETHIC_percentages_df1 = [\n",
    "        (ETHIC_ind1_df1 * 100 / total_topicos_ETHIC),\n",
    "        (ETHIC_grup_df1 * 100 / total_topicos_ETHIC),\n",
    "        (ETHIC_ind2_df1 * 100 / total_topicos_ETHIC),\n",
    "    ]\n",
    "    ETHIC_percentages_df2 = [\n",
    "        (ETHIC_ind1_df2 * 100 / total_topicos_ETHIC),\n",
    "        (ETHIC_grup_df2 * 100 / total_topicos_ETHIC),\n",
    "        (ETHIC_ind2_df2 * 100 / total_topicos_ETHIC),\n",
    "    ]\n",
    "\n",
    "    print(\"Generación de gráficos...\")  \n",
    "    # Grafico 1, topicos distintos por etapa para BERT\n",
    "    # Diferencial 1\n",
    "    color_graphd1 = 'lightblue'\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(['Ind1', 'Grup', 'Ind2'], [BERT_ind1_df1, BERT_grup_df1, BERT_ind2_df1], color=color_graphd1)\n",
    "    plt.bar(['Ind1', 'Grup', 'Ind2'], [BERT_ind1_df1, BERT_grup_df1, BERT_ind2_df1], color=color_graphd1)\n",
    "    plt.title(f\"Caso: {caso} - Tópicos BERT distintos por etapa, Diferencial 1\", fontsize=16, fontfamily='serif')\n",
    "    plt.xlabel('Etapa', fontsize=14)\n",
    "    plt.ylabel('Tópicos Distintos', fontsize=14)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Agregar porcentajes sobre las barras\n",
    "    for bar, percentage in zip(bars, BERT_percentages_df1):\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1, f'{percentage:.1f}%', ha='center', fontsize=12)\n",
    "   \n",
    "    #plt.tight_layout()\n",
    "    plt.figtext(0.5, 0.01, f'Total de tópicos distintos: {total_topicos_BERT}', ha='center', fontsize=12)\n",
    "    plt.savefig(f'../resultados/{caso}/BERT_Distinct_Topics_D1.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Diferencial 2\n",
    "    color_graphd2 = 'lightgreen'\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(['Ind1', 'Grup', 'Ind2'], [BERT_ind1_df2, BERT_grup_df2, BERT_ind2_df2], color=color_graphd2)\n",
    "    plt.bar(['Ind1', 'Grup', 'Ind2'], [BERT_ind1_df2, BERT_grup_df2, BERT_ind2_df2], color=color_graphd2)\n",
    "    plt.title(f\"Caso: {caso} - Tópicos BERT distintos por etapa, Diferencial 2\", fontsize=16, fontfamily='serif')\n",
    "    plt.xlabel('Etapa', fontsize=14)\n",
    "    plt.ylabel('Tópicos Distintos', fontsize=14)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Agregar porcentajes sobre las barras\n",
    "    for bar, percentage in zip(bars, BERT_percentages_df2):\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1, f'{percentage:.1f}%', ha='center', fontsize=12)\n",
    "    \n",
    "    #plt.tight_layout()\n",
    "    plt.figtext(0.5, 0.01, f'Total de tópicos distintos: {total_topicos_BERT}', ha='center', fontsize=12)\n",
    "    plt.savefig(f'../resultados/{caso}/BERT_Distinct_Topics_D2.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # Grafico 2, topicos distintos por etapa para ETHIC \n",
    "    # Diferencial 1\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(['Ind1', 'Grup', 'Ind2'], [ETHIC_ind1_df1, ETHIC_grup_df1, ETHIC_ind2_df1], color=color_graphd1)\n",
    "    plt.bar(['Ind1', 'Grup', 'Ind2'], [ETHIC_ind1_df1, ETHIC_grup_df1, ETHIC_ind2_df1], color=color_graphd1)\n",
    "    plt.title(f\"Caso: {caso} - Tópicos éticos distintos por etapa, Diferencial 1\", fontsize=16, fontfamily='serif')\n",
    "    plt.xlabel('Etapa', fontsize=14)\n",
    "    plt.ylabel('Tópicos Distintos', fontsize=14)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    for bar, percentage in zip(bars, ETHIC_percentages_df1):\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1, f'{percentage:.1f}%', ha='center', fontsize=12)\n",
    "    \n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.figtext(0.5, 0.01, f'Total de tópicos distintos: {total_topicos_ETHIC}', ha='center', fontsize=12)\n",
    "    plt.savefig(f'../resultados/{caso}/ETHIC_Distinct_Topics_D1.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Diferencial 2\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(['Ind1', 'Grup', 'Ind2'], [ETHIC_ind1_df2, ETHIC_grup_df2, ETHIC_ind2_df2], color=color_graphd2)\n",
    "    plt.bar(['Ind1', 'Grup', 'Ind2'], [ETHIC_ind1_df2, ETHIC_grup_df2, ETHIC_ind2_df2], color=color_graphd2)\n",
    "    plt.title(f\"Caso: {caso} - Tópicos éticos distintos por etapa, Diferencial 2\", fontsize=16, fontfamily='serif')\n",
    "    plt.xlabel('Etapa', fontsize=14)\n",
    "    plt.ylabel('Tópicos Distintos', fontsize=14)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    for bar, percentage in zip(bars, ETHIC_percentages_df2):\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1, f'{percentage:.1f}%', ha='center', fontsize=12)\n",
    "    \n",
    "    #plt.tight_layout()\n",
    "    plt.figtext(0.5, 0.01, f'Total de tópicos distintos: {total_topicos_ETHIC}', ha='center', fontsize=12)\n",
    "    plt.savefig(f'../resultados/{caso}/ETHIC_Distinct_Topics_D2.png')\n",
    "    plt.close()\n",
    "\n",
    "    print(\"Gráficos generados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Realizando conteo de tópicos BERT y ETHIC distintos en cada etapa...\n",
      "Conteo finalizado\n",
      "Generación de gráficos...\n",
      "Gráficos generados\n"
     ]
    }
   ],
   "source": [
    "distinct_topics(caso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación de tópicos éticos entre etapas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ethic_topics_between_stages(caso):\n",
    "    print(\"=== Comparación de Tópicos Éticos entre Etapas ===\")\n",
    "    df1 = pd.read_csv(f\"../processed_data/{caso}/ETHIC_Topics_df1.csv\")\n",
    "    df2 = pd.read_csv(f\"../processed_data/{caso}/ETHIC_Topics_df2.csv\")\n",
    "    print(\"Generando gráfico...\")\n",
    "    # Contar tópicos en Ind1 e Ind2 para df1\n",
    "    df1['len_ETHIC_topicos_ind1'] = df1['ETHIC_topicos_ind1'].apply(len)\n",
    "    df1['len_ETHIC_topicos_ind2'] = df1['ETHIC_topicos_ind2'].apply(len)\n",
    "    df1['topicos_ind2_mayor'] = df1['len_ETHIC_topicos_ind2'] > df1['len_ETHIC_topicos_ind1']\n",
    "    DF1 = df1['topicos_ind2_mayor'].sum()\n",
    "\n",
    "    # Contar tópicos en Ind1 e Ind2 para df2\n",
    "    df2['len_ETHIC_topicos_ind1'] = df2['ETHIC_topicos_ind1'].apply(len)\n",
    "    df2['len_ETHIC_topicos_ind2'] = df2['ETHIC_topicos_ind2'].apply(len)\n",
    "    df2['topicos_ind2_mayor'] = df2['len_ETHIC_topicos_ind2'] > df2['len_ETHIC_topicos_ind1']\n",
    "    DF2 = df2['topicos_ind2_mayor'].sum()\n",
    "\n",
    "    # Datos para el gráfico\n",
    "    data = {\n",
    "        'Diferencial': ['Diferencial 1', 'Diferencial 2'],\n",
    "        'Frecuencia': [DF1, DF2]\n",
    "    }\n",
    "    df_plot = pd.DataFrame(data)\n",
    "\n",
    "    # Total estudiantes df1 y df2\n",
    "    total_alumnos1 = len(df1)\n",
    "    total_alumnos2 = len(df2)\n",
    "\n",
    "    # Crear el gráfico\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    barplot = sns.barplot(\n",
    "        x='Diferencial', \n",
    "        y='Frecuencia', \n",
    "        data=df_plot, \n",
    "        hue='Diferencial', \n",
    "        palette={'Diferencial 1': 'lightblue', 'Diferencial 2': 'lightgreen'},\n",
    "        dodge=False\n",
    "    )\n",
    "\n",
    "    # Añadir valores en las barras\n",
    "    totals = [total_alumnos1, total_alumnos2]  # Totales de estudiantes para cada diferencial\n",
    "    for p, total in zip(barplot.patches, totals):\n",
    "        percentage = 100 * p.get_height() / total  # Calcular el porcentaje\n",
    "        barplot.annotate(\n",
    "            f'{percentage:.1f}%',  # Mostrar el porcentaje con un decimal\n",
    "            (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "            ha='center', \n",
    "            va='center', \n",
    "            fontsize=12, \n",
    "            color='black', \n",
    "            xytext=(0, 10), \n",
    "            textcoords='offset points'\n",
    "        )\n",
    "\n",
    "    # Personalizar el gráfico\n",
    "    plt.title(f\"Caso: {caso} - Estudiantes con más tópicos éticos en la etapa individual 2 que en la individual 1, por diferencial\", \n",
    "              fontsize=18, fontfamily='serif', weight='bold', pad=20)\n",
    "    plt.xlabel(\"Diferencial\", fontsize=14, labelpad=10)\n",
    "    plt.ylabel(\"Número de estudiantes\", fontsize=14, labelpad=10)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.figtext(0.5, 0.01, \n",
    "                f\"Total de estudiantes diferencial 1: {total_alumnos1} | Total de estudiantes diferencial 2: {total_alumnos2}\", \n",
    "                ha=\"center\", fontsize=12, color='black')\n",
    "    \n",
    "    # Guardar gráfico\n",
    "    #plt.tight_layout()\n",
    "    plt.savefig(f\"../resultados/{caso}/ETHIC_Topicos_Ind2_Mayor_Ind1.png\", dpi=300)\n",
    "    plt.close()\n",
    "    print(\"Gráfico generado exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ethic_topics_dependency_between_stages(caso):\n",
    "    print(\"=== Dependencia de Tópicos Éticos entre Etapas ===\")\n",
    "    df1 = pd.read_csv(f\"../processed_data/{caso}/ETHIC_Topics_df1.csv\")\n",
    "    df2 = pd.read_csv(f\"../processed_data/{caso}/ETHIC_Topics_df2.csv\")\n",
    "    print(\"Generando gráfico...\")\n",
    "\n",
    "    # Asegurarse de que las columnas relevantes son listas\n",
    "    df1['ETHIC_topicos_ind1'] = df1['ETHIC_topicos_ind1'].apply(eval)  \n",
    "    df1['ETHIC_topicos_grup'] = df1['ETHIC_topicos_grup'].apply(eval)\n",
    "    df1['ETHIC_topicos_ind2'] = df1['ETHIC_topicos_ind2'].apply(eval)\n",
    "\n",
    "    df2['ETHIC_topicos_ind1'] = df2['ETHIC_topicos_ind1'].apply(eval)\n",
    "    df2['ETHIC_topicos_grup'] = df2['ETHIC_topicos_grup'].apply(eval)\n",
    "    df2['ETHIC_topicos_ind2'] = df2['ETHIC_topicos_ind2'].apply(eval)\n",
    "\n",
    "    # Función para verificar dependencia de tópicos\n",
    "    def check_dependency(row, ind_col, grup_col, target_col):\n",
    "        return any(\n",
    "            topic in row[grup_col] and topic not in row[ind_col] \n",
    "            for topic in row[target_col]\n",
    "        )\n",
    "\n",
    "    # Dependencia en Diferencial 1\n",
    "    df1['tiene_topicos_dep'] = df1.apply(\n",
    "        lambda row: check_dependency(row, 'ETHIC_topicos_ind1', 'ETHIC_topicos_grup', 'ETHIC_topicos_ind2'), \n",
    "        axis=1\n",
    "    )\n",
    "    DF1_dependency_count = df1['tiene_topicos_dep'].sum()\n",
    "\n",
    "    # Dependencia en Diferencial 2\n",
    "    df2['tiene_topicos_dep'] = df2.apply(\n",
    "        lambda row: check_dependency(row, 'ETHIC_topicos_ind1', 'ETHIC_topicos_grup', 'ETHIC_topicos_ind2'), \n",
    "        axis=1\n",
    "    )\n",
    "    DF2_dependency_count = df2['tiene_topicos_dep'].sum()\n",
    "\n",
    "    # Totales de estudiantes\n",
    "    total_alumnos1 = len(df1)\n",
    "    total_alumnos2 = len(df2)\n",
    "\n",
    "    # Datos para el gráfico\n",
    "    data = {\n",
    "        'Diferencial': ['Diferencial 1', 'Diferencial 2'],\n",
    "        'Número de estudiantes': [DF1_dependency_count, DF2_dependency_count],\n",
    "    }\n",
    "    df_plot = pd.DataFrame(data)\n",
    "\n",
    "    # Crear el gráfico\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    barplot = sns.barplot(\n",
    "        x='Diferencial', \n",
    "        y='Número de estudiantes', \n",
    "        data=df_plot, \n",
    "        hue='Diferencial',  \n",
    "        palette={'Diferencial 1': 'lightblue', 'Diferencial 2': 'lightgreen'},\n",
    "        dodge=False  \n",
    "    )\n",
    "\n",
    "    # Añadir valores en las barras (en porcentaje)\n",
    "    totals = [total_alumnos1, total_alumnos2]       # Totales de estudiantes\n",
    "    for p, total in zip(barplot.patches, totals):\n",
    "        percentage = 100 * p.get_height() / total   # Calcular porcentaje\n",
    "        barplot.annotate(\n",
    "            f'{percentage:.1f}%',  # Formato porcentaje\n",
    "            (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "            ha='center', \n",
    "            va='center', \n",
    "            fontsize=12, \n",
    "            color='black', \n",
    "            xytext=(0, 10), \n",
    "            textcoords='offset points'\n",
    "        )\n",
    "\n",
    "    # Personalizar el gráfico\n",
    "    plt.title(\n",
    "        f\"Caso: {caso} - Cantidad de estudiantes que añadieron nuevos tópicos éticos en la etapa individual 2 a partir de la grupal\", \n",
    "        fontsize=16, fontfamily='serif', weight='bold', pad=20\n",
    "    )\n",
    "    plt.xlabel(\"Diferencial\", fontsize=14, labelpad=10)\n",
    "    plt.ylabel(\"Número de estudiantes\", fontsize=14, labelpad=10)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Añadir texto descriptivo\n",
    "    plt.figtext(\n",
    "        0.5, 0.01, \n",
    "        f\"Total de estudiantes diferencial 1: {total_alumnos1} | Total de estudiantes diferencial 2: {total_alumnos2}\\nEste ánalisis representa topicos por estudiante encontrados en la etapa individual 2 y grupal, que no estaban previamente en la etapa individual 1, es decir,\\n tópicos nuevos luego de comentar entre pares.\", \n",
    "        ha=\"center\", fontsize=14, color='black'\n",
    "    )\n",
    "\n",
    "    # Guardar gráfico\n",
    "    plt.savefig(f\"../resultados/{caso}/ETHIC_Topicos_Dependencia_Ind2_no_Ind1_Grup.png\")\n",
    "    plt.close()\n",
    "    print(\"Gráfico generado exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comparación de Tópicos Éticos entre Etapas ===\n",
      "Generando gráfico...\n",
      "Gráfico generado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "ethic_topics_between_stages(caso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dependencia de Tópicos Éticos entre Etapas ===\n",
      "Generando gráfico...\n",
      "Gráfico generado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "ethic_topics_dependency_between_stages(caso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nube de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import numpy as np\n",
    "\n",
    "def procesar_comentarios(comentarios, tokenizer):\n",
    "    texto = ' '.join(comentarios.dropna())  \n",
    "    tokens = tokenizer(texto)  \n",
    "    return ' '.join(tokens)  \n",
    "\n",
    "def generar_nube_palabras(caso, texto, titulo, colormap='viridis', background_color='white', max_words=200, mask=None, stop_words_custom=None):\n",
    "    wordcloud = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color=background_color,\n",
    "        colormap=colormap,  \n",
    "        max_words=max_words,\n",
    "        contour_color='steelblue',  \n",
    "        contour_width=1,\n",
    "        mask=mask, \n",
    "        stopwords=STOPWORDS.union(stop_words_custom if stop_words_custom else set()) \n",
    "    ).generate(texto)\n",
    "    \n",
    "    # Mostrar la nube de palabras\n",
    "    plt.figure(figsize=(10, 5), facecolor=None)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(titulo, fontsize=16, color='darkblue', fontfamily='serif')\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig(f\"../resultados/{caso}/WC_{titulo}.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Función principal para generar nubes de palabras\n",
    "def crear_nubes_palabras(caso, df_diferencial_1, df_diferencial_2, tokenizer, stop_words_custom=None, colormap='plasma', background_color='ivory'):\n",
    "    # Crear nubes de palabras para Diferencial 1\n",
    "    for columna in ['Comentario - Ind1 - Diferencial 1', 'Comentario - Grup - Diferencial 1', 'Comentario - Ind2 - Diferencial 1']:\n",
    "        texto_procesado = procesar_comentarios(df_diferencial_1[columna], tokenizer)\n",
    "        generar_nube_palabras(caso, texto_procesado, f\"Nube de Palabras para {columna}\", colormap=colormap, background_color=background_color, stop_words_custom=stop_words_custom)\n",
    "    # Crear nubes de palabras para Diferencial 2\n",
    "    for columna in ['Comentario - Ind1 - Diferencial 2', 'Comentario - Grup - Diferencial 2', 'Comentario - Ind2 - Diferencial 2']:\n",
    "        texto_procesado = procesar_comentarios(df_diferencial_2[columna], tokenizer)\n",
    "        generar_nube_palabras(caso, texto_procesado, f\"Nube de Palabras para caso: {columna}\", colormap=colormap, background_color=background_color, stop_words_custom=stop_words_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop_words_custom' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstop_words_custom\u001b[49m\n\u001b[0;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m StemmerTokenizer(stem\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, lemmatize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m crear_nubes_palabras(caso, df_df1, df_df2, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, stop_words_custom\u001b[38;5;241m=\u001b[39mstop_words_custom)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stop_words_custom' is not defined"
     ]
    }
   ],
   "source": [
    "stop_words_custom\n",
    "tokenizer = StemmerTokenizer(stem=True, lemmatize=True)\n",
    "crear_nubes_palabras(caso, df_df1, df_df2, tokenizer=tokenizer, stop_words_custom=stop_words_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frecuencia de aparición de palabras éticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_y_preprocesar_comentarios(df_dif1, df_dif2, tokenizer):\n",
    "    columnas_comentarios_dif1 = [\n",
    "        'Comentario - Ind1 - Diferencial 1',\n",
    "        'Comentario - Grup - Diferencial 1',\n",
    "        'Comentario - Ind2 - Diferencial 1'\n",
    "    ]\n",
    "    columnas_comentarios_dif2 = [\n",
    "        'Comentario - Ind1 - Diferencial 2',\n",
    "        'Comentario - Grup - Diferencial 2',\n",
    "        'Comentario - Ind2 - Diferencial 2'\n",
    "    ]\n",
    "    \n",
    "    comentarios_dif1 = df_dif1[columnas_comentarios_dif1].fillna('').values.flatten()\n",
    "    comentarios_dif2 = df_dif2[columnas_comentarios_dif2].fillna('').values.flatten()\n",
    "    comentarios = pd.concat([pd.Series(comentarios_dif1), pd.Series(comentarios_dif2)], axis=0).values.flatten()\n",
    "    comentarios = [str(c) for c in comentarios]\n",
    "    return [' '.join(tokenizer(c)) for c in comentarios]\n",
    "\n",
    "# Leer las palabras éticas desde un archivo\n",
    "def read_ethic_words(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return set(f.read().splitlines())\n",
    "    \n",
    "# Contar cuantas palabras éticas hay en los comentarios: cada palabra etica es contabilizada una sola vez por comentario\n",
    "def contar_palabras_etica(df1, df2, tokenizer):\n",
    "    palabras_etica = read_ethic_words('../dictionaries/ethic_words.txt')\n",
    "    comentarios = cargar_y_preprocesar_comentarios(df1, df2, tokenizer)\n",
    "    palabras_etica = [tokenizer(palabra)[0] for palabra in palabras_etica]\n",
    "    contador = Counter()\n",
    "    \n",
    "    for comentario in comentarios:\n",
    "        for palabra in comentario.split():\n",
    "            if palabra in palabras_etica:\n",
    "                contador[palabra] += 1\n",
    "                \n",
    "    palabras, frecuencias = zip(*contador.items())\n",
    "    total_comentarios = len(comentarios)\n",
    "    \n",
    "    # Calcular porcentajes\n",
    "    porcentajes = [frecuencia * 100 / total_comentarios for frecuencia in frecuencias]\n",
    "    \n",
    "    # Crear el DataFrame\n",
    "    df_frecuencias = pd.DataFrame({\n",
    "        'Palabra': palabras,\n",
    "        'Frecuencia': frecuencias,\n",
    "        'Porcentaje': porcentajes\n",
    "    })\n",
    "    \n",
    "    # Sort dataframe by Frecuencia & Porcentaje\n",
    "    df_frecuencias.sort_values(by=['Frecuencia', 'Porcentaje'], ascending=[False, False], inplace=True)\n",
    "\n",
    "    # Guardar el DataFrame\n",
    "    df_frecuencias.to_csv(f\"../processed_data/{caso}/ETHIC_WORDS_freq_{caso}.csv\", index=False)\n",
    "\n",
    "    # Graficar\n",
    "    total_estudiantes = len(df1) + len(df2)\n",
    "\n",
    "    # Graficar\n",
    "    plt.figure(figsize=(50, 30))\n",
    "    ax = sns.barplot(\n",
    "        x='Frecuencia',\n",
    "        y='Palabra',\n",
    "        data=df_frecuencias,\n",
    "        palette='viridis'\n",
    "    )\n",
    "    plt.title(f\"Caso: {caso} - Frecuencia de palabras éticas en comentarios\", fontsize=16, fontfamily='serif')\n",
    "    plt.xlabel('Frecuencia', fontsize=14)\n",
    "    plt.ylabel('Palabras Éticas', fontsize=14)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.figtext(0.75, 0.01, f\"Total de estudiantes: {total_estudiantes} | Total de respuestas: {total_comentarios}\" , wrap=True, horizontalalignment='center', fontsize=12)\n",
    "\n",
    "    # Agregar etiquetas\n",
    "    for i, bar in enumerate(ax.patches):\n",
    "        # Obtener frecuencia y porcentaje directamente del DataFrame ordenado\n",
    "        frecuencia = df_frecuencias.iloc[i]['Frecuencia']\n",
    "        porcentaje = df_frecuencias.iloc[i]['Porcentaje']\n",
    "        \n",
    "        plt.text(\n",
    "            bar.get_width() + 12,  # Ajustar posición horizontal\n",
    "            bar.get_y() + bar.get_height() / 2,  # Centrar verticalmente\n",
    "            f\"{int(frecuencia)} ({porcentaje:.1f}%)\",  # Etiqueta\n",
    "            ha='left', va='center', fontsize=10, color='black'\n",
    "        )\n",
    "\n",
    "    plt.savefig(f\"../resultados/{caso}/ETHIC_WORDS_freq_{caso}.png\", dpi=300)\n",
    "\n",
    "    # Graficar TOP 10\n",
    "    top10 = df_frecuencias.head(10)\n",
    "\n",
    "    plt.figure(figsize=(20, 12)) \n",
    "    ax_top10 = sns.barplot(\n",
    "        x='Frecuencia',\n",
    "        y='Palabra',\n",
    "        data=top10,\n",
    "        palette='viridis'\n",
    "    )\n",
    "\n",
    "    plt.title(f\"Caso: {caso} - Top 10 palabras éticas\", fontsize=16, fontfamily='serif')\n",
    "    plt.xlabel('Frecuencia', fontsize=14)\n",
    "    plt.ylabel('Palabras Éticas', fontsize=14)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Agregar etiquetas para el TOP 10\n",
    "    for i, bar in enumerate(ax_top10.patches):\n",
    "        frecuencia = top10.iloc[i]['Frecuencia']\n",
    "        porcentaje = top10.iloc[i]['Porcentaje']\n",
    "        plt.text(\n",
    "            bar.get_width() + 2,  # Posición más ajustada\n",
    "            bar.get_y() + bar.get_height() / 2,\n",
    "            f\"{int(frecuencia)} ({porcentaje:.1f}%)\",\n",
    "            ha='left', va='center', fontsize=10, color='black'\n",
    "        )\n",
    "\n",
    "    plt.figtext(0.75, 0.01, f\"Total de estudiantes: {total_estudiantes} | Total de respuestas: {total_comentarios}\" , wrap=True, horizontalalignment='center', fontsize=12)\n",
    "    plt.savefig(f\"../resultados/{caso}/ETHIC_WORDS_TOP10_{caso}.png\", dpi=300)\n",
    "\n",
    "    return df_frecuencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "stop_words_custom\n",
    "tokenizer = StemmerTokenizer(stem=True, lemmatize=True)\n",
    "contar_palabras_etica(df_df1, df_df2, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conectores más usados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Cargar el modelo de Spacy en español\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "def identificar_conectores(texto):\n",
    "    \"\"\"Identifica conectores en el texto dado.\"\"\"\n",
    "    doc = nlp(texto)\n",
    "    conectores_en_texto = [token.text.lower() for token in doc if token.pos_ in [\"CCONJ\", \"SCONJ\", \"ADP\"]]\n",
    "    return conectores_en_texto\n",
    "\n",
    "def extract_grammar_connectors(df1, df2, caso):\n",
    "    # Leer el archivo de datos\n",
    "    conectores_totales = []\n",
    "\n",
    "    # Extraer conectores de cada columna\n",
    "    Ind1_d1 = df1['Comentario - Ind1 - Diferencial 1'].apply(identificar_conectores).tolist()\n",
    "    Grup_d1 = df1['Comentario - Grup - Diferencial 1'].apply(identificar_conectores).tolist()\n",
    "    Ind2_d1 = df1['Comentario - Ind2 - Diferencial 1'].apply(identificar_conectores).tolist()\n",
    "\n",
    "    Ind1_d2 = df2['Comentario - Ind1 - Diferencial 2'].apply(identificar_conectores).tolist()\n",
    "    Grup_d2 = df2['Comentario - Grup - Diferencial 2'].apply(identificar_conectores).tolist()\n",
    "    Ind2_d2 = df2['Comentario - Ind2 - Diferencial 2'].apply(identificar_conectores).tolist()\n",
    "\n",
    "    conectores_totales.extend([conector for sublist in Ind1_d1 for conector in sublist])\n",
    "    conectores_totales.extend([conector for sublist in Grup_d1 for conector in sublist])\n",
    "    conectores_totales.extend([conector for sublist in Ind2_d1 for conector in sublist])\n",
    "\n",
    "    conectores_totales.extend([conector for sublist in Ind1_d2 for conector in sublist])\n",
    "    conectores_totales.extend([conector for sublist in Grup_d2 for conector in sublist])\n",
    "    conectores_totales.extend([conector for sublist in Ind2_d2 for conector in sublist])\n",
    "\n",
    "    # Calcular la frecuencia de conectores\n",
    "    frecuencia_conectores = Counter(conectores_totales)\n",
    "    df_frecuencia = pd.DataFrame(frecuencia_conectores.items(), columns=['Conectores', 'Frecuencia'])\n",
    "    df_frecuencia = df_frecuencia.sort_values(by='Frecuencia', ascending=False)\n",
    "    df_frecuencia = df_frecuencia[df_frecuencia['Frecuencia'] > 1]\n",
    "\n",
    "    # Guardar conectores y sus frecuencias en un JSON\n",
    "    df_frecuencia.to_json('../dictionaries/conectores_frecuencia.json', orient='records')\n",
    "\n",
    "    # Crear el directorio si no existe\n",
    "    os.makedirs(f\"../resultados/{caso}\", exist_ok=True)\n",
    "\n",
    "    # Crear una tabla con Matplotlib\n",
    "    fig, ax = plt.subplots(figsize=(10, 27.5)) # figsize ejes x, y\n",
    "    # Ajustar márgenes para reducir la distancia entre elementos\n",
    "    ax.axis('off') \n",
    "    table = ax.table(cellText=df_frecuencia.values, colLabels=df_frecuencia.columns, cellLoc='center', loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.auto_set_column_width(col=list(range(len(df_frecuencia.columns))))\n",
    "    table.scale(3, 3)  \n",
    "   \n",
    "    for (i, j), cell in table.get_celld().items():\n",
    "        if i == 0:  # Encabezados\n",
    "            cell.set_fontsize(12)\n",
    "            cell.set_text_props(weight='bold')\n",
    "            cell.set_facecolor('#cccccc') \n",
    "        # Alternar colores de fondo para mejorar la legibilidad\n",
    "        elif i % 2 == 0:\n",
    "            cell.set_facecolor('#f5f5f5')  # Color gris claro para filas pares\n",
    "        else:\n",
    "            cell.set_facecolor('#ffffff')  # Blanco para filas impares\n",
    "        # Agregar un borde más visible a las celdas\n",
    "        cell.set_edgecolor('black')\n",
    "            \n",
    "    total_conectores = df_frecuencia['Frecuencia'].sum()\n",
    "    total_comentarios = len(df1)*3 + len(df2)*3\n",
    "    total_estudiantes = len(df1) + len(df2)\n",
    "    plt.figtext(0.5, 0.01, f\"Total de estudiantes: {total_estudiantes} | Total de respuestas: {total_comentarios} | Total de conectores: {total_conectores}\", ha=\"center\", fontsize=12)\n",
    "    # Guardar la tabla como imagen\n",
    "    plt.title(f'Caso: {caso} - Frecuencia de Conectores en Comentarios', fontsize=16, fontfamily='serif')\n",
    "    plt.savefig(f\"../resultados/{caso}/Conectores_frecuencia_tabla.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "extract_grammar_connectors(df_df1, df_df2, caso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Justificaciones post conectores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "def identificar_conectores_y_frases(texto):\n",
    "    doc = nlp(texto)\n",
    "    conectores_y_frases = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in [\"CCONJ\", \"SCONJ\", \"ADP\"]:\n",
    "            frase = ' '.join([t.text.lower() for t in doc[token.i + 1:token.i + 6]])  # Captura hasta 5 palabras después del conector\n",
    "            conectores_y_frases.append((token.text, frase))\n",
    "    return conectores_y_frases\n",
    "\n",
    "def extract_keywords_after_connectors(df1, df2, caso):\n",
    "    conectores_y_frases_totales = []\n",
    "\n",
    "    # Extraer conectores y frases de las columnas de df1 para Diferencial 1\n",
    "    for column in ['Comentario - Ind1 - Diferencial 1', 'Comentario - Grup - Diferencial 1', 'Comentario - Ind2 - Diferencial 1']:\n",
    "        conectores_y_frases_totales.extend(df1[column].apply(identificar_conectores_y_frases).tolist())\n",
    "\n",
    "    # Extraer conectores y frases de las columnas de df2 para Diferencial 2\n",
    "    for column in ['Comentario - Ind1 - Diferencial 2', 'Comentario - Grup - Diferencial 2', 'Comentario - Ind2 - Diferencial 2']:\n",
    "        conectores_y_frases_totales.extend(df2[column].apply(identificar_conectores_y_frases).tolist())\n",
    "\n",
    "    # Aplanar la lista de listas\n",
    "    conectores_y_frases_totales = [item for sublist in conectores_y_frases_totales for item in sublist]\n",
    "\n",
    "    # Contar frecuencia de conectores\n",
    "    conectores = [item[0] for item in conectores_y_frases_totales]\n",
    "    frecuencia_conectores = Counter(conectores)\n",
    "    conectores_mas_usados = [item[0] for item in frecuencia_conectores.most_common(10)]\n",
    "    frases_relevantes = [frase for conector, frase in conectores_y_frases_totales if conector in conectores_mas_usados]\n",
    "\n",
    "    # Calcular TF-IDF para identificar palabras clave en las frases relevantes\n",
    "    custom_stopwords = list(cargar_stopwords('../dictionaries/stopwords_es.txt'))\n",
    "    vectorizer = TfidfVectorizer(stop_words=custom_stopwords, max_features=30)  \n",
    "    tfidf_matrix = vectorizer.fit_transform(frases_relevantes)\n",
    "\n",
    "    # Obtener palabras clave\n",
    "    palabras_clave = vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "    keywords = sorted(zip(palabras_clave, tfidf_scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Convertir palabras clave en DataFrame\n",
    "    df_keywords = pd.DataFrame(keywords, columns=['Palabra Clave', 'Relevancia'])\n",
    "    # Calcular el mínimo y el máximo de las importancias\n",
    "    min_importancia = df_keywords['Relevancia'].min()\n",
    "    max_importancia = df_keywords['Relevancia'].max()\n",
    "\n",
    "    # Estandarización de 0 a 1\n",
    "    df_keywords['Estandarizado'] = (df_keywords['Relevancia'] - min_importancia) / (max_importancia - min_importancia)\n",
    "\n",
    "    # Opcionalmente, redondear las columnas para mayor claridad\n",
    "    df_keywords['Estandarizado'] = df_keywords['Estandarizado'].round(2)\n",
    "    df_keywords['Relevancia'] = df_keywords['Relevancia'].round(2)\n",
    "\n",
    "    # Guardar el DataFrame como CSV\n",
    "    os.makedirs(f\"../resultados/{caso}\", exist_ok=True)\n",
    "    df_keywords.to_csv(f\"../processed_data/{caso}/Palabras_Clave_Despues_Conectores.csv\", index=False)\n",
    "\n",
    "    # Crear un gráfico de tabla para las palabras y su importancia\n",
    "    plt.figure(figsize=(10, 13))\n",
    "    plt.axis('tight')\n",
    "    plt.axis('off')\n",
    "\n",
    "    tabla = plt.table(cellText=df_keywords.values,\n",
    "                      colLabels=df_keywords.columns,\n",
    "                      cellLoc='center',\n",
    "                      loc='center')\n",
    "    tabla.auto_set_font_size(False)\n",
    "    tabla.set_fontsize(12)\n",
    "    tabla.scale(2, 2)\n",
    "\n",
    "    # Estilo de celdas\n",
    "    for i, key in enumerate(df_keywords.index):\n",
    "        for j, col in enumerate(df_keywords.columns):\n",
    "            if i == 0:  # Encabezados\n",
    "                tabla[(i,j)].set_fontsize(12)\n",
    "                tabla[(i,j)].set_text_props(weight='bold')\n",
    "                tabla[(i,j)].set_facecolor('#cccccc')  # Fondo gris claro\n",
    "            # Alternar colores de fondo para mejorar la legibilidad\n",
    "            elif i % 2 == 0:\n",
    "                tabla[(i, j)].set_facecolor('#f5f5f5')  # Color gris claro para filas pares\n",
    "            else:\n",
    "                tabla[(i, j)].set_facecolor('#ffffff')  # Blanco para filas impares\n",
    "            # Agregar un borde más visible a las celdas\n",
    "            tabla[(i, j)].set_edgecolor('black')\n",
    "    \n",
    "    plt.title(f\"Caso: {caso} - Palabras post conectores\", fontsize=14, fontfamily='serif')\n",
    "    plt.figtext(0.5, 0.01, f\"Total de estudiantes: {len(df1)+len(df2)} | Total de respuestas: {len(df1)*3+len(df2)*3}\", ha=\"center\", fontsize=12)\n",
    "    plt.savefig(f\"../resultados/{caso}/Conectores_Palabras_Clave.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Palabras clave después de conectores guardadas en ../resultados/{caso}/Palabras_Clave_Despues_Conectores.csv\")\n",
    "    return df_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "extract_keywords_after_connectors(df_df1, df_df2, caso)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ethics_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
